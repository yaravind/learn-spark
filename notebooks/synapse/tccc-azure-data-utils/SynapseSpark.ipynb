{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import com.tccc.dna.synapse.spark.SynapseSpark  \n",
        "import org.apache.spark.sql.DataFrame\n",
        "import com.tccc.dna.synapse.AzStorage\n",
        "import com.microsoft.spark.notebook.msutils.MSFileInfo\n",
        "\n",
        "  def deepLsToDataFrame(rootStr: String): DataFrame = {\n",
        "    val root = MSFileInfo(name = null, path = rootStr, size = 0, isDir = true, isFile = false, modifyTime = System.currentTimeMillis)\n",
        "    val files = AzStorage.deepLs(root)\n",
        "\n",
        "    // sc.parallelize() method creates a distributed and parallelized dataset on existing collection on the \"driver\"\n",
        "    val fileDf = spark.createDataFrame(spark.sparkContext.parallelize(files))\n",
        "    fileDf\n",
        "  }\n",
        "\n",
        "val filePath = \"aem/asset-created/2023/11/20/\"\n",
        "val storageAcct = \"xxx\"\n",
        "val containerOrFileSys = \"dam-adobe\"\n",
        "val landingPath = f\"abfss://$containerOrFileSys@$storageAcct.dfs.core.windows.net/$filePath\""
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## deepLsToDataFrame"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full ABFSS URI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val fileListDf = deepLsToDataFrame(landingPath)\n",
        "display(fileListDf)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.hadoop.fs.{FileSystem, Path, LocatedFileStatus, RemoteIterator}\n",
        "def deepLs1(path: String): Unit = {\n",
        "    val iter: RemoteIterator[LocatedFileStatus] = FileSystem\n",
        "      .get(SynapseSpark.getActiveSession.sparkContext.hadoopConfiguration)\n",
        "      .listFiles(new Path(path), true)\n",
        "\n",
        "    if(!iter.hasNext) println(\"No files!\")\n",
        "    while (iter.hasNext) {\n",
        "      val fs: LocatedFileStatus = iter.next()\n",
        "      println(fs.toString)\n",
        "    }\n",
        "  }\n",
        "deepLs1(\"user\")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val landingPathGlob = f\"abfss://$containerOrFileSys@$storageAcct.dfs.core.windows.net/aem/asset-created/2023/11/**/\"\n",
        "val fileListDf1 = deepLsToDataFrame(landingPathGlob)\n",
        "display(fileListDf1)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relative path starting with \"/\""
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val fileListDf2 = deepLsToDataFrame(\"/user\")\n",
        "display(fileListDf2)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}