{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Synapse has Azure Open Datasets package pre-installed. You will use [NYC Yellow Taxi trip records](https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=azureml-opendatasets) data in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Load nyc green taxi trip records from azure open dataset\n",
        "val blob_account_name = \"azureopendatastorage\"\n",
        "\n",
        "val nyc_blob_container_name = \"nyctlc\"\n",
        "val nyc_blob_relative_path = \"yellow\"\n",
        "val nyc_blob_sas_token = \"\"\n",
        "val nycYellowPath = \"/poc/delatalake/nyc_yellow_taxi_trips\"\n",
        "\n",
        "val nyc_wasbs_path = f\"wasbs://$nyc_blob_container_name@$blob_account_name.blob.core.windows.net/$nyc_blob_relative_path\"\n",
        "spark.conf.set(f\"fs.azure.sas.$nyc_blob_container_name.$blob_account_name.blob.core.windows.net\",nyc_blob_sas_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "DELETE FROM delta.`/poc/ddl/person`;\n",
        "DELETE FROM delta.`/poc/ddl/person_df_write`;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create\n",
        "\n",
        "A delta table can be created in 2 ways\n",
        "\n",
        "1. Write a dataframe to storage in Delta format or\n",
        "2. Explicitly create a table definition in catalog and define the schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1. Writte a dataframe to storage in Delta format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "// Write dataframe to storage\n",
        "{\n",
        "    val nyc_tlc_df = spark.read.parquet(nyc_wasbs_path)\n",
        "    nyc_tlc_df.write.partitionBy(\"puYear\").format(\"delta\").save(nycYellowPath)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Explicitly create table definition in catalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2.1 Use `saveAsTable`\n",
        "\n",
        "Define the table within the metastore using the `saveAsTable` method or a `CREATE TABLE` statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "// Create manage delta table\n",
        "val nyc_tlc_df = spark.read.format(\"delta\").load(nycYellowPath)\n",
        "nyc_tlc_df.write.partitionBy(\"puYear\").format(\"delta\").saveAsTable(\"test.delta_nyc_yellow_taxi_trips\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2.2 Use `CREATE TABLE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%sql\n",
        "\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS person (\n",
        "    id INT NOT NULL, \n",
        "    name STRING NOT NULL, \n",
        "    crt_ts TIMESTAMP NOT NULL, \n",
        "    crt_usr STRING NOT NULL, \n",
        "    UPD_ts TIMESTAMP, \n",
        "    UPD_usr STRING\n",
        ") \n",
        "LOCATION '/poc/ddl/'\n",
        "COMMENT 'Student external table'\n",
        "TBLPROPERTIES ('createBy'='Notebook', 'createdOn'='2023-06-20', 'app'='Customer Satisfaction', 'dataZone'='refined', 'pipelineRunId'='UUID', 'notebook'='nb_ddl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS delta.`/poc/ddl/person` (\n",
        "    id INT NOT NULL, \n",
        "    name STRING NOT NULL, \n",
        "    crt_ts TIMESTAMP NOT NULL, \n",
        "    crt_usr STRING NOT NULL, \n",
        "    upd_ts TIMESTAMP, \n",
        "    upd_usr STRING\n",
        ") \n",
        "USING DELTA\n",
        "COMMENT 'Person unmanaged table'\n",
        "TBLPROPERTIES ('type' = 'unmanaged', 'createBy'='Notebook', 'createdOn'='2023-06-20', 'app'='Customer Satisfaction', 'dataZone'='refined', 'pipelineRunId'='UUID', 'notebook'='nb_ddl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import java.sql._\n",
        "import scala.collection.mutable\n",
        "import org.apache.spark.sql.{DataFrame, Row}\n",
        "import org.apache.spark.sql.functions.current_timestamp\n",
        "import org.apache.spark.sql.types._\n",
        "import io.delta.tables.DeltaTable\n",
        "\n",
        "val TargetSchema = StructType(Seq(\n",
        "    StructField(\"id\", IntegerType, nullable = false),\n",
        "    StructField(\"name\", StringType, nullable = false),\n",
        "    StructField(\"crt_ts\", TimestampType, nullable = true),\n",
        "    StructField(\"crt_usr\", StringType, nullable = true),\n",
        "    StructField(\"upd_ts\", TimestampType, nullable = true),\n",
        "    StructField(\"upd_usr\", StringType, nullable = true)\n",
        "  ))\n",
        "\n",
        "def createDataframeFromCollection(schema: StructType, data: Seq[Row]) = {\n",
        "    spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n",
        "}\n",
        "\n",
        "def upsert(targetPath: String, sourceDf: DataFrame, sourceExcludeCols: List[String] = List(),\n",
        "             mergeOnCols: List[String], partitionPruneCols: List[String] = List(),\n",
        "             customInsertExpr: Map[String, String] = Map(), customUpdateExpr: Map[String, String] = Map()): Unit = {\n",
        "    println(s\"targetPath: $targetPath, sourceExcludeCols: $sourceExcludeCols, mergeOnCols: $mergeOnCols\")\n",
        "    val targetTable = DeltaTable.forPath(spark, targetPath)\n",
        "    val targetDf = targetTable.toDF\n",
        "    println(s\"Target table cols: (${targetDf.columns.length})\" + targetDf.columns.mkString(\", \"))\n",
        "    println(s\"Source Dataframe cols: (${sourceDf.columns.length})\" + sourceDf.columns.mkString(\", \"))\n",
        "\n",
        "    val mergeCols = sourceDf.columns.filter(col => !sourceExcludeCols.contains(col))\n",
        "\n",
        "    val updateExpr = mergeCols.map(colName => (s\"target.$colName\", s\"source.$colName\")).toMap\n",
        "    val finalUpdateExpr = updateExpr ++ customUpdateExpr\n",
        "\n",
        "    val insertExpr: mutable.Map[String, String] = mutable.Map[String, String]() ++= finalUpdateExpr\n",
        "    val finalInsertExpr = insertExpr ++ customInsertExpr\n",
        "\n",
        "    println(s\"updateExpr: $finalUpdateExpr\")\n",
        "    println(s\"insertExpr: $finalInsertExpr\")\n",
        "\n",
        "    val mergeCondition = mergeOnCols.map(col => s\"target.$col = source.$col\").mkString(\" AND \")\n",
        "    println(s\"mergeCondition: $mergeCondition\")\n",
        "\n",
        "    targetTable.as(\"target\")\n",
        "      .merge(sourceDf.as(\"source\"), mergeCondition)\n",
        "      .whenMatched\n",
        "      .updateExpr(finalUpdateExpr)\n",
        "      .whenNotMatched\n",
        "      .insertExpr(finalInsertExpr)\n",
        "      .execute()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Initial load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val baseDf = createDataframeFromCollection(TargetSchema, Seq(\n",
        "                    //id, name, crt_ts, crt_usr, upd_ts, upd_usr\n",
        "                    Row(1, \"Alice\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user1\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user1\"),\n",
        "                    Row(2, \"Bob\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user2\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user2\")\n",
        "                ))\n",
        "\n",
        "//Nothing to update\n",
        "val updateMap: Map[String, String] = Map()\n",
        "\n",
        "//Do not carry upd_ts, upd_usr from source\n",
        "val excludeCols = List(\"upd_ts\", \"upd_usr\")\n",
        "\n",
        "//Modify crt_ts\n",
        "val insertMap: Map[String, String] = Map(\n",
        "    \"target.crt_ts\" -> s\"${current_timestamp()}\",\n",
        "    \"target.crt_usr\" -> s\"'${mssparkutils.env.getUserName()}'\"\n",
        ")\n",
        "\n",
        "upsert(\"/poc/ddl/person\", baseDf, sourceExcludeCols = excludeCols, mergeOnCols = List(\"id\"), partitionPruneCols = List(), \n",
        "    customInsertExpr = insertMap, customUpdateExpr = updateMap)\n",
        "\n",
        "println(\"Complete upsert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT * FROM delta.`/poc/ddl/person`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "DESCRIBE TABLE EXTENDED delta.`/poc/ddl/person`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Upsert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val SourceSchema = StructType(Seq(\n",
        "      StructField(\"id\", IntegerType, nullable = false),\n",
        "      StructField(\"name\", StringType, nullable = false),\n",
        "      StructField(\"crt_ts\", TimestampType, nullable = true),\n",
        "      StructField(\"crt_usr\", StringType, nullable = true)\n",
        "))\n",
        "\n",
        "val sourceDf = createDataframeFromCollection(SourceSchema, Seq(\n",
        "      Row(1, \"Alice-update\", Timestamp.valueOf(\"2023-01-02 00:00:00\"), \"user1\"), //update\n",
        "      Row(3, \"Charlie-insert\", Timestamp.valueOf(\"2023-01-02 00:00:00\"), \"user2\") //insert\n",
        "    ))\n",
        "\n",
        "//Modify the upd_ts and upd_usr for merged rows\n",
        "val updateCols = Map(\n",
        "    \"target.upd_ts\" -> s\"${current_timestamp()}\",\n",
        "    \"target.upd_usr\" -> \"'testUser'\"\n",
        ")\n",
        "\n",
        "upsert(\"/poc/ddl/person\", sourceDf,\n",
        "    sourceExcludeCols = List(),\n",
        "    mergeOnCols = List(\"id\"), partitionPruneCols = List(), customUpdateExpr = updateCols)\n",
        "  println(\"Complete upsert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT * FROM delta.`/poc/ddl/person`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val df = spark.read.format(\"delta\").load(\"/poc/ddl/person\")\n",
        "df.printSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read\n",
        "\n",
        "1. Read directly from storage\n",
        "2. Read from metastore defined table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1. Read directly from storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "// Read directly from storage\n",
        "{\n",
        "    val df = spark.read.format(\"delta\").load(nycYellowPath).take(2)\n",
        "    display(df)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT * from delta.`/poc/delatalake/nyc_yellow_taxi_trips` LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Read from metastore defined table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT * from test.delta_nyc_yellow_taxi_trips LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Caveats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Nullability handling \n",
        "\n",
        "Handling nulls depends on the method used to create the table. Using `CREATE TABLE` or `saveAsTable` table describes the schema in catalog hence hanoring the non-null columns. If the Dataframe is written to ADLS using `df.write` then all coumns are treated as `nullable = true`.\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "Writing in parquet, the underlying format of delta lake, can't guarantee the nullability of the column.\n",
        "\n",
        "Maybe you wrote a parquet that for sure it's not null, but the schema is never validated on write in parquet, and any could append some data with the same schema, but with nulls. **So spark will always put as nullable the columns, just to prevention.**\n",
        "\n",
        "This behavior can be prevented using a catalog, that will validate that the dataframe follows the expected schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val testDf = createDataframeFromCollection(TargetSchema, Seq(\n",
        "                    //id, name, crt_ts, crt_usr, upd_ts, upd_usr\n",
        "                    Row(1, \"Alice\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user1\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user1\"),\n",
        "                    Row(2, \"Bob\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user2\", Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user2\")\n",
        "                ))\n",
        "testDf.write.format(\"delta\").save(\"/poc/ddl/person_df_write\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Schema when described in catalog using saveAsTable or CREATE TABLE:\")\n",
        "spark.read.format(\"delta\").load(\"/poc/ddl/person\").printSchema\n",
        "\n",
        "println(\"Schema when df.write is used:\")\n",
        "spark.read.format(\"delta\").load(\"/poc/ddl/person_df_write\").printSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Write test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create dataframe with `name` column as `null`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val NullableSchema = StructType(Seq(\n",
        "    StructField(\"id\", IntegerType, nullable = false),\n",
        "    StructField(\"name\", StringType, nullable = true), //Change name to nullable\n",
        "    StructField(\"crt_ts\", TimestampType, nullable = true),\n",
        "    StructField(\"crt_usr\", StringType, nullable = true),\n",
        "    StructField(\"upd_ts\", TimestampType, nullable = true),\n",
        "    StructField(\"upd_usr\", StringType, nullable = true)\n",
        "  ))\n",
        "\n",
        "val nullDf = createDataframeFromCollection(NullableSchema, Seq(\n",
        "                    //id, name, crt_ts, crt_usr, upd_ts, upd_usr\n",
        "                    Row(100, null, Timestamp.valueOf(\"2023-01-01 00:00:00\"), \"user1\", null, null)\n",
        "                ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Save to `/poc/ddl/person_df_write` should succeed as the table wasn't decribed in catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "upsert(\"/poc/ddl/person_df_write\", nullDf,\n",
        "    sourceExcludeCols = List(),\n",
        "    mergeOnCols = List(\"id\"), partitionPruneCols = List(), customUpdateExpr = updateCols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT * from delta.`/poc/ddl/person_df_write`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Save to `/poc/ddl/person` should FAIL as the table IS decribed in catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "upsert(\"/poc/ddl/person\", nullDf,\n",
        "    sourceExcludeCols = List(),\n",
        "    mergeOnCols = List(\"id\"), partitionPruneCols = List(), customUpdateExpr = updateCols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT * from delta.`/poc/ddl/person`"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}