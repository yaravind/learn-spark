{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ACID - Atomicity Guarantee\n",
        "\n",
        "Atomic guarantees through Transactional Log `_delta_log`. Transaction log is the **single source of the truth** for your table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## LogStore\n",
        "\n",
        "Object storage systems generally [doesn't provide capabilities](https://docs.delta.io/0.8.0/delta-storage.html#microsoft-azure-storage) required for atomic guarantees out-of-the-box. So, Delta Lake transactional operations typically go through the LogStore API implementattion instead of accessing the storage system directly. `LogStore is an abstraction of transaction log stores (to read and write Delta log files).` Spark Pool uses the following implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(spark.conf.get(\"spark.delta.logStore.class\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Working with transaction logs\n",
        "\n",
        "When a Delta table is created\n",
        "\n",
        "- A new folder is created where all the data and corresponding metadata is stored\n",
        "- A sub-directory, called `_delta_log`, is created to store the transaction log\n",
        "- All the files under that folder togeter constitute the table's transaction log\n",
        "- Every change (CRUD) to the table is recorded as ordered atomic commits under that folder\n",
        "- Each commit is a written out as a JSON file, starting with `000000.json`\n",
        "- Each numeric JSON file increment represents a **new version** of the table\n",
        "- Delta Lake creates a **checkpoint file in Parquet** format after every 10 JSON files or after every 10th commit (i.e. transaction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "val deltaLogPath = \"/poc/delatalake/nyc_yellow_taxi_trips/_delta_log/00000000000000000000.json\"\n",
        "val transLogDf = spark.read.json(deltaLogPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "transLogDf.printSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "{\n",
        "    val addDf = transLogDf.select(\"add\").filter(\"add IS NOT NULL\")\n",
        "    display(addDf.limit(2))\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### commitInfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "{\n",
        "    val commitInfoDf = transLogDf.select(\"commitInfo\").filter(\"commitInfo IS NOT NULL\")\n",
        "    display(commitInfoDf)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "{\n",
        "    val metadataDf = transLogDf.select(\"metadata\").filter(\"metadata IS NOT NULL\")\n",
        "    display(metadataDf)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### protocol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "{\n",
        "    val protocolDf = transLogDf.select(\"protocol\").filter(\"protocol IS NOT NULL\")\n",
        "    display(protocolDf)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Working with checkpoint file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "{\n",
        "    //spark.read.parquet(deltaLogPath)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}