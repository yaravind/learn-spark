{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "!pip install azure-storage-file-datalake"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pyarrowfs_adlgen2. It is isn't available in internal package manager so can't add it to requirements.txt"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyarrowfs_adlgen2"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AccessToken\n",
        "from notebookutils import mssparkutils\n",
        "import pyarrow.fs\n",
        "import pyarrowfs_adlgen2\n",
        "from azure.identity import ManagedIdentityCredential\n",
        "from datetime import datetime\n",
        "\n",
        "class SynapseAuthToken:\n",
        "    def get_token(*args, **kwargs):\n",
        "        # credential = ManagedIdentityCredential()\n",
        "        # credential._credential = SynapseAuthToken()\n",
        "        dt = datetime.today()\n",
        "        seconds = dt.timestamp()\n",
        "\n",
        "        return AccessToken(\n",
        "            token=mssparkutils.credentials.getToken(audience=\"storage\"),\n",
        "            # some random time in the future... synapse doesn't provide way to get an expires_on value for a token\n",
        "            expires_on=seconds + (60 * 100)\n",
        "        )\n",
        "\n",
        "\n",
        "def __get_azure_filesystem(storage_acc_name, container_name):\n",
        "    credential = ManagedIdentityCredential()\n",
        "    credential._credential = SynapseAuthToken()\n",
        "\n",
        "    # Use the credential from spoof_token class\n",
        "    handler = FilesystemHandler.from_account_name(\n",
        "        storage_acc_name, container_name, credential)\n",
        "\n",
        "    fs: FilesystemHandler = pq.fs.PyFileSystem(handler)\n",
        "\n",
        "def get_parquet_metadata(file_path, storage=Storage.Local, storage_acc_name=None, container_name=None):\n",
        "    if storage == Storage.Local:\n",
        "        pq_file: ParquetFile = pq.ParquetFile(file_path)\n",
        "    else:\n",
        "        file_sys_handler = __get_azure_filesystem(storage_acc_name, container_name)\n",
        "        pq_file: ParquetFile = pq.ParquetFile(file_path, filesystem=file_sys_handler)\n",
        "\n",
        "    # Return the metadata components as a tuple\n",
        "    return (\n",
        "        get_file_meta(pq_file),\n",
        "        get_schema(pq_file),\n",
        "        get_row_groups_meta(pq_file),\n",
        "        get_columns_meta(pq_file),\n",
        "        get_columns_stats(pq_file)\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pkg_resources\n",
        "\n",
        "installed_packages = pkg_resources.working_set\n",
        "for package in sorted([f\"{i.key}=={i.version}\" for i in installed_packages]):\n",
        "    print(package)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from metadata_utils import get_parquet_metadata, get_parquet_metadata_json, get_parquet_metadata_df\n",
        "\n",
        "def print_parquet_metadata(file_path, print_json=False, print_dataframe=True):\n",
        "    file_summary, schema, row_groups, columns, column_stats = get_parquet_metadata(file_path)\n",
        "    if print_json:\n",
        "        file_summary_json, schema_json, row_groups_json, columns_json, column_stats_json = get_parquet_metadata_json(\n",
        "            file_path)\n",
        "    if print_dataframe:\n",
        "        file_summary_df, schema_df, row_groups_df, columns_df, column_stats_df = get_parquet_metadata_df(file_path)\n",
        "\n",
        "    # 1. File summary\n",
        "    print(\"\\nFile Summary:\")\n",
        "    print(\"\\t\", file_summary_json) if print_json else None\n",
        "    print(file_summary_df) if print_dataframe else None\n",
        "\n",
        "    #  2. Schema\n",
        "    print(\"\\nSchema:\")\n",
        "    print(\"\\t\", schema_json) if print_json else None\n",
        "    print(schema_df) if print_dataframe else None\n",
        "\n",
        "    # 3. Row group summary\n",
        "    print(\"\\nRow Groups Summary: \", len(row_groups))\n",
        "    print(\"\\t\", row_groups_json) if print_json else None\n",
        "    print(row_groups_df) if print_dataframe else None\n",
        "\n",
        "    # 4. Column metadata\n",
        "    print(\"\\nColumn Summary: \")\n",
        "    print(\"\\t\", columns_json) if print_json else None\n",
        "    print(columns_df) if print_dataframe else None\n",
        "\n",
        "    # 5.  Column stats\n",
        "    print(\"\\nColumn Statistics: \")\n",
        "    print(\"\\t\", column_stats_json) if print_json else None\n",
        "    print(column_stats_df) if print_dataframe else None"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pq_path = '/Users/o60774/Documents/WS/datasets/new_york_taxi/parquet/'\n",
        "pq_path='abfss://tlfs@xxx.dfs.core.windows.net/poc/parquet/nyc_yellow_taxi_trips/puYear=2001/puMonth=1/part-00493-127a558e-137b-4f6b-8c6e-904dc74a264e.c000.snappy.parquet'\n",
        "\n",
        "import io\n",
        "from azure.storage.blob import BlockBlobService\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Replace these variables with your actual account_name and account_key\n",
        "ACCOUNT_NAME = \"xxx\"\n",
        "ACCOUNT_KEY = \"\"\n",
        "\n",
        "# Replace this with the container name and Parquet file path in ADLS Gen2\n",
        "CONTAINER_NAME = \"tlfs\"\n",
        "PARQUET_FILE_PATH = 'poc/parquet/nyc_yellow_taxi_trips/puYear=2001/puMonth=1/part-00493-127a558e-137b-4f6b-8c6e-904dc74a264e.c000.snappy.parquet'\n",
        "\n",
        "# Create a BlockBlobService instance to interact with blob storage\n",
        "block_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
        "\n",
        "# Read the Parquet file as a stream\n",
        "with io.BytesIO() as parquet_stream:\n",
        "    block_blob_service.get_blob_to_stream(CONTAINER_NAME, PARQUET_FILE_PATH, parquet_stream)\n",
        "    parquet_stream.seek(0)\n",
        "\n",
        "    # Open the Parquet file using ParquetFile class to get metadata\n",
        "    parquet_file = pq.ParquetFile(parquet_stream)\n",
        "    \n",
        "    # Retrieve and print metadata\n",
        "    metadata = parquet_file.metadata\n",
        "    print(\"Metadata:\\n\", metadata)\n",
        "\n",
        "\n",
        "\n",
        "#print_parquet_metadata(pq_path + 'part-00113.snappy.parquet', print_dataframe=True)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace these variables with your actual account_name and account_key\n",
        "ACCOUNT_NAME = \"xxx\"\n",
        "ACCOUNT_KEY = \"\"\n",
        "\n",
        "# Replace this with the container name and Parquet file path in ADLS Gen2\n",
        "CONTAINER_NAME = \"tlfs\"\n",
        "PARQUET_FILE_PATH = 'poc/parquet/nyc_yellow_taxi_trips/puYear=2001/puMonth=1/part-00493-127a558e-137b-4f6b-8c6e-904dc74a264e.c000.snappy.parquet'\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential\n",
        "import os, uuid\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AzureNamedKeyCredential\n",
        "from azure.core.exceptions import HttpResponseError\n",
        "from azure.storage.filedatalake import (\n",
        "    AnalyticsLogging,\n",
        "    CorsRule,\n",
        "    DataLakeDirectoryClient,\n",
        "    DataLakeFileClient,\n",
        "    DataLakeServiceClient,\n",
        "    FileSystemClient,\n",
        "    Metrics,\n",
        "    RetentionPolicy,\n",
        "    StaticWebsite)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "account_url = f\"https://{ACCOUNT_NAME}.blob.core.windows.net\"\n",
        "\n",
        "try:\n",
        "    print(\"Azure Blob Storage Python quickstart sample\")\n",
        "\n",
        "    default_credential = DefaultAzureCredential()\n",
        "    # Create the BlobServiceClient object\n",
        "    blob_service_client = BlobServiceClient(account_url, credential=default_credential)\n",
        "    #stream = blob_service_client.download_blob()\n",
        "\n",
        "except ResourceNotFoundError:\n",
        "    print(\"No blob found.\")\n",
        "except Exception as ex:\n",
        "    print('Exception:')\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_client = DataLakeServiceClient.from_connection_string(\n",
        "        f\"DefaultEndpointsProtocol=https;AccountName={ACCOUNT_NAME};AccountKey={ACCOUNT_KEY};EndpointSuffix=core.windows.net\")\n",
        "\n",
        "file_system_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)\n",
        "\n",
        "directory_client = file_system_client.get_directory_client(\"poc/parquet/nyc_yellow_taxi_trips/puYear=2001/puMonth=1\")\n",
        "\n",
        "file_client = directory_client.get_file_client(\"part-00493-127a558e-137b-4f6b-8c6e-904dc74a264e.c000.snappy.parquet\")\n",
        "\n",
        "download = file_client.download_file()\n",
        "\n",
        "downloaded_bytes = download.readall()\n",
        "\n",
        "#with open(\"./sample.txt\", \"wb\") as my_file:\n",
        "#    my_file.write(downloaded_bytes)\n",
        "#    my_file.close()"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.filedatalake import DataLakeFileClient\n",
        "\n",
        "file = DataLakeFileClient.from_connection_string(f\"DefaultEndpointsProtocol=https;AccountName={ACCOUNT_NAME};AccountKey={ACCOUNT_KEY};EndpointSuffix=core.windows.net\",\n",
        "                                                 file_system_name=CONTAINER_NAME, file_path=\"poc/parquet/nyc_yellow_taxi_trips/puYear=2001/puMonth=1\")\n",
        "\n",
        "with open(\"./part-00493-127a558e-137b-4f6b-8c6e-904dc74a264e.c000.snappy.parquet\", \"wb\") as my_file:\n",
        "    download = file.download_file()\n",
        "    download.readinto(my_file)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}