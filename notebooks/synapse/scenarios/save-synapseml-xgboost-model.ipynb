{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Configure SynapseML based on doc\n",
        "\n",
        "https://microsoft.github.io/SynapseML/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"name\": \"synapseml\",\n",
        "  \"conf\": {\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.11.4-spark3.3\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\",\n",
        "      \"spark.sql.parquet.enableVectorizedReader\": \"false\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Doesn't compile without loading the jars explicitly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "    \"conf\": {\n",
        "        \"spark.jars\": \"abfss://metadata@storagemai01us2dev.dfs.core.windows.net/komsdriver/jar_files/json4s-native_2.12-3.5.3.jar,abfss://metadata@storagemai01us2dev.dfs.core.windows.net/komsdriver/jar_files/xgboost4j_2.12-1.4.1.jar,abfss://metadata@storagemai01us2dev.dfs.core.windows.net/komsdriver/jar_files/xgboost4j-spark_2.12-1.4.1.jar\",\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Train, Predict and Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor\n",
        "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.hadoop.fs.FileSystem\n",
        " \n",
        "// Create a SparkSession\n",
        "// val spark = SparkSession.builder()\n",
        "//   .appName(\"XGBoostRegressorSample\")\n",
        "//   .getOrCreate()\n",
        " \n",
        "// Sample data\n",
        "val data = Seq(\n",
        "  (1.0, 2.0, 3.0),\n",
        "  (2.0, 3.0, 4.0),\n",
        "  (3.0, 4.0, 5.0),\n",
        "  (4.0, 5.0, 6.0),\n",
        "  (5.0, 6.0, 7.0)\n",
        ")\n",
        " \n",
        "// Define schema for sample data\n",
        "val schema = List(\"feature1\", \"feature2\", \"label\")\n",
        " \n",
        "// Create DataFrame from sample data\n",
        "val df = spark.createDataFrame(data).toDF(schema: _*)\n",
        " \n",
        "// Define feature columns and label column\n",
        "val featureCols = Array(\"feature1\", \"feature2\")\n",
        "val labelCol = \"label\"\n",
        " \n",
        "// Create a VectorAssembler to combine feature columns into a single vector column\n",
        "val assembler = new VectorAssembler()\n",
        "  .setInputCols(featureCols)\n",
        "  .setOutputCol(\"features\")\n",
        " \n",
        "// Transform the data using VectorAssembler\n",
        "val assembledDF = assembler.transform(df)\n",
        " \n",
        "// Define XGBoost parameters\n",
        "val paramMap = Map(\n",
        "  \"eta\" -> 0.1,\n",
        "  \"max_depth\" -> 2,\n",
        "  \"objective\" -> \"reg:linear\",\n",
        "  \"num_round\" -> 100,\n",
        "  \"early_stopping_rounds\" -> 10\n",
        ")\n",
        " \n",
        "// Create XGBoostRegressor\n",
        "val xgbRegressor = new XGBoostRegressor(paramMap)\n",
        "  .setLabelCol(labelCol)\n",
        "  .setFeaturesCol(\"features\")\n",
        " \n",
        "// Train XGBoost model\n",
        "val xgbModel = xgbRegressor.fit(assembledDF)\n",
        " \n",
        "//Make predictions\n",
        "val predictions = xgbModel.transform(assembledDF)\n",
        " \n",
        "// Show predictions\n",
        "predictions.show()\n",
        " \n",
        "// Save model to primary storage account\n",
        "xgbModel.write.overwrite().save(\"MS_DRIVER_MODEL/test001.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import com.tccc.dna.synapse.spark.Classpath  \n",
        "{\n",
        "    Classpath.getAllDependencies.foreach(println)\n",
        "}"
      ]
    }
  ]
}