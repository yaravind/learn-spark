{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Partitioning in Spark\n",
        "\n",
        "Partitioning is important for following reasons\n",
        "\n",
        "- It affects parallelism\n",
        "- It affects resiliency: Failure in computing even a single record will result in retry of the whole partition.\n",
        "- It affects data shuffling\n",
        "- It affects operations efficiency\n",
        "- It affects the RAM processing: `mapPartitions` and `foreachPartition` work at partition level. Therefore, if there are more records per partition then there is a more demand on available RAM.\n",
        "- It affects data skew\n",
        "- It affects storage in files: Usally produces one file for each partition."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.commons.lang3.time.{DateFormatUtils, FastDateFormat}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql._\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.log4j._\n",
        "import com.tccc.dna.synapse.dataset.NewYorkTaxiYellow\n",
        "import com.tccc.dna.synapse.{StorageFormat, AzStorage}\n",
        "import com.tccc.dna.synapse.Logs._\n",
        "\n",
        "import com.tccc.dna.synapse.spark.{SynapseSpark, DataFrames, Partitions, Writers, Catalogs}\n",
        "\n",
        "import java.nio.charset.StandardCharsets\n",
        "import java.util.Date\n",
        "\n",
        "sc.setLogLevel(\"DEBUG\")\n",
        "\n",
        "val notebookName = SynapseSpark.getCurrentNotebookName\n",
        "val log = org.apache.log4j.LogManager.getLogger(s\"com.aravind.notebook.$notebookName\")\n",
        "log.setLevel(Level.DEBUG)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "The setup and initialization logic is refactored into the below notebook."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run /Operations/00-Setup-NewYorkYellowTaxi"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partitions while reading from Source\n",
        "\n",
        "- When reading from the source, **the # of partitions are determined** by pre-built rules. These rules determine the number of partitions of the input RDD. The input source can be a runtime collection of records, a set of files from filesystem or Table in a database.\n",
        "- The files or part of a file from the source are then mapped to these pre-determined partitions.\n",
        "- However, partitionng on existing RDD is accomplished using `Partitioner` implementation.\n",
        "\n",
        "Rules:\n",
        "- Number of CPU Cores per Spark executor is defined by `spark.executor.cores` configuration.\n",
        "- Single CPU Core can read one file or partition of a splittable file at a single point in time.\n",
        "- Once read a file is transformed into one or multiple partitions in memory.\n",
        "\n",
        "**_Procedure for selecting input partitions \"of a Dataset\" while reading from source:_**\n",
        "\n",
        "1. _Step 1: Calculate and identify file chunks: Split individual files at the boundary of `maxSplitBytes` if the file is splittable._\n",
        "2. _Step 2: Partition packing i.e. one or more file chunks are packed into a partition. During packing of chunks into a partition, if the partition size exceeds `maxSplitBytes` then the partition is considered complete for packing and a new partition is taken for packing remaining chunks._\n",
        "\n",
        "```\n",
        "maxSplitBytes = Minimum(spark.sql.files.maxPartitionBytes [default 128 MB], bytesPerCore)\n",
        "bytesPerCore = (Sum of sizes of all files + No. of files * spark.sql.files.openCostInBytes [default 4 MB])/spark.default.parallelism\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizations\n",
        "\n",
        "Optimizing read parallelism:\n",
        "- If number of cores is equal to the number of files, files are not splittable and some of them are larger in size - larger files become a bottleneck, Cores responsible for reading smaller files will idle for some time.\n",
        "- If there are more Cores than the number of files - Cores that do not have files assigned to them will Idle. If we do not perform repartition after reading the files - the cores will remain Idle during processing stages.\n",
        "\n",
        "**Rule of thumb:** \n",
        "\n",
        "- Set number of Cores to be two times less than files being read. Adjust according to your situation.\n",
        "- You can use `spark.sql.files.maxPartitionBytes` config to set maximum size of the partition when reading files. Files that are larger will be split into multiple partitions accordingly."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SynapseSpark.printPartitionSplitRelatedProps()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell prints `None` because there is **NO partitioner** while reading from source. Pre-built rules are used."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//Cache as it is used in rest of the notebook\n",
        "val statgeTaxiDeltaDf = DataFrames.getDataFrame(tcccStorageAcct, tcccContainer, yellowTaxiDeltaPath, StorageFormat.Delta).cache\n",
        "logDebug(log, s\"Partitioner used: ${statgeTaxiDeltaDf.rdd.partitioner}\")\n",
        "//Create a view to work using SQL\n",
        "statgeTaxiDeltaDf.createOrReplaceTempView(\"stage_nyc_yellow_taxi_trips_delta\")\n",
        "\n",
        "val stageTaxiDelta2001Df = statgeTaxiDeltaDf.filter(\"puYear == 2001\")\n",
        "logDebug(log, s\"Partitioner used: ${stageTaxiDelta2001Df.rdd.partitioner}\")\n",
        "\n",
        "val shuffledTaxiDf = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        puYear, COUNT(*) as trips_count \n",
        "    FROM \n",
        "        stage_nyc_yellow_taxi_trips_delta \n",
        "    GROUP BY puYear \n",
        "    ORDER BY puYear\"\"\")\n",
        "logDebug(log, s\"Partitioner used: ${shuffledTaxiDf.rdd.partitioner}\")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partitions in Memory\n",
        "\n",
        "- By default spark creates partitions equal to the number of CPU cores in cluster\n",
        "- Spark also creates 1 Task per partition\n",
        "- Shuffle operations move data from one partition to other partitions. By default, spark shuffle operations create `200` partitions\n",
        "- `repartitionXXX()` and `coalesce()` are generally used to changes partitions runtime in-memory"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    //Partitions from open dataset\n",
        "    val yellowTaxiOpenDf = NewYorkTaxiYellow.getOpenDatasetDataFrame\n",
        "    val inMemPartitionCount = Partitions.getNumPartitionsInMem(yellowTaxiOpenDf)\n",
        "    logDebug(log, s\"Open dataset In-memory partition count: $inMemPartitionCount\")\n",
        "    var minMaxPartDf = Partitions.getRecordsPerPartitionInMem(yellowTaxiOpenDf)\n",
        "    minMaxPartDf.agg(min(\"record_count\"), max(\"record_count\")).show\n",
        "\n",
        "    //Partitions from local storage acct\n",
        "    var df = DataFrames.getDataFrame(tcccStorageAcct, tcccContainer, yellowTaxiCsvPath, StorageFormat.Csv)\n",
        "    logDebug(log, s\"$yellowTaxiCsvPath In-memory partition count: ${Partitions.getNumPartitionsInMem(df)}\")\n",
        "    minMaxPartDf = Partitions.getRecordsPerPartitionInMem(df)\n",
        "    minMaxPartDf.agg(min(\"record_count\"), max(\"record_count\")).show\n",
        "\n",
        "    df = DataFrames.getDataFrame(tcccStorageAcct, tcccContainer, yellowTaxiParquetPath, StorageFormat.Parquet)\n",
        "    logDebug(log, s\"$yellowTaxiParquetPath In-memory partition count: ${Partitions.getNumPartitionsInMem(df)}\")\n",
        "    minMaxPartDf = Partitions.getRecordsPerPartitionInMem(df)\n",
        "    minMaxPartDf.agg(min(\"record_count\"), max(\"record_count\")).show\n",
        "\n",
        "    //cache this \n",
        "    df = DataFrames.getDataFrame(tcccStorageAcct, tcccContainer, yellowTaxiDeltaPath, StorageFormat.Delta)\n",
        "    logDebug(log, s\"$yellowTaxiDeltaPath In-memory partition count: ${Partitions.getNumPartitionsInMem(df)}\")\n",
        "    minMaxPartDf = Partitions.getRecordsPerPartitionInMem(df)\n",
        "    minMaxPartDf.agg(min(\"record_count\"), max(\"record_count\")).show\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partitions during Processing\n",
        "\n",
        "Use `spark.default.parallelism` and `spark.sql.shuffle.partitions` configurations to set the number of partitions created after performing **wide** transformations.\n",
        "\n",
        "**Rule of thumb:** set `spark.default.parallelism` equal to `spark.executor.cores` **X** the number of executors **X** a small number from 2 to 8, tune to specific Spark job."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "--SHOW PARTITIONS stage_nyc_yellow_taxi_trips_delta;\n",
        "--SELECT puYear, puMonth, COUNT(*) as trips_count FROM stage_nyc_yellow_taxi_trips_delta GROUP BY puYear, puMonth ORDER BY puYear, puMonth;\n",
        "SELECT puYear, COUNT(*) as trips_count FROM stage_nyc_yellow_taxi_trips_delta GROUP BY puYear ORDER BY puYear;"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partitions on Disk"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tables: SQL Managed Table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    logDebug(log, s\"\\nPartition info for $yellowTaxiParquetBackedTable\")\n",
        "    logDebug(log, s\"Is paritioned: ${Partitions.isTablePartitioned(schemaName, yellowTaxiParquetBackedTable)}\")\n",
        "    var partitionCols = Partitions.getTablePartitionCols(schemaName, yellowTaxiParquetBackedTable)\n",
        "    logDebug(log, s\"Paritioned on col(s): $partitionCols\")\n",
        "    println()\n",
        "    logDebug(log, s\"\\nPartition info for $yellowTaxiDeltaBackedTable\")\n",
        "    logDebug(log, s\"Is paritioned: ${Partitions.isTablePartitioned(schemaName, yellowTaxiDeltaBackedTable)}\")\n",
        "    partitionCols = Partitions.getTablePartitionCols(schemaName, yellowTaxiDeltaBackedTable)\n",
        "    logDebug(log, s\"Paritioned on col(s): $partitionCols\")\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "--describe extended silver.mt_nyc_yellow_taxi_trips_delta\n",
        "describe extended silver.mt_nyc_yellow_taxi_trips_parquet"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val fullName = schemaName+\".\"+yellowTaxiDeltaBackedTable"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "--DESCRIBE EXTENDED silver.mt_nyc_yellow_taxi_trips_delta;\n",
        "--DESCRIBE FORMATTED silver.mt_nyc_yellow_taxi_trips_delta;\n",
        "--DESCRIBE DETAIL silver.mt_nyc_yellow_taxi_trips_delta;"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//Overwrite partition 2001\n",
        "//incrementalOverwrite(trips2001Df, schemaName, managedTableName)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//incrementalAppend(trips2001Df, schemaName, managedTableName)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select count(*) from silver.c;\n",
        "select count(*) from silver.mt_nyc_yellow_taxi_trips_delta;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Writers.saveAsManagedTable(taxiOpenDatasetDf, schemaName, managedTableName, partitioCols, \"Data of New York Yello Taxi Trips. There are about 1.5B rows (50 GB) in total as of 2018.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val descDf=spark.sql(s\"DESCRIBE TABLE EXTENDED $schemaName.$yellowTaxiDeltaBackedTable\")//.show(truncate = false,numRows = 1000)\n",
        "display(descDf)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- All Delta queries fail with the following error becuase SHOW PARTITIONS command is not supported yet for Delta.\n",
        "--Error: Table spark_catalog.delta.`abfss://tlfs@xxx.dfs.core.windows.net/synapse/workspaces/syn-tccc-cdl-use2-dev-01/warehouse/silver.db/mt_nyc_yellow_taxi_trips_delta` does not support partition management.;\n",
        "--SHOW partitions delta.`abfss://tlfs@xxx.dfs.core.windows.net/synapse/workspaces/syn-tccc-cdl-use2-dev-01/warehouse/silver.db/mt_nyc_yellow_taxi_trips_delta`;\n",
        "-- show partitions silver.mt_nyc_yellow_taxi_trips_delta;\n",
        "--SHOW PARTITIONS silver.mt_nyc_yellow_taxi_trips_parquet\n",
        "--DESCRIBE FORMATTED silver.mt_nyc_yellow_taxi_trips_delta"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File: CSV"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logDebug(log, s\"\\nPartition info for $yellowTaxiCsvPath\")\n",
        "logDebug(log, s\"Is paritioned: ${Partitions.isCsvFilePartitioned(yellowTaxiCsvPath)}\")\n",
        "logDebug(log, s\"Paritioned on col(s): ${Partitions.getCsvFilePartitionCols(yellowTaxiCsvPath)}\")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File: Parquet"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// TODO DOESN't accurately work\n",
        "/*logDebug(log, s\"\\nPartition info for $yellowTaxiParquetPath\")\n",
        "//logDebug(log, s\"Is paritioned: ${isParquetFilePartitioned(yellowTaxiParquetPath)}\")\n",
        "logDebug(log, s\"Is paritioned: ${isParquetFilePartitioned(\"/poc/parquet\")}\")\n",
        "//logDebug(log, s\"Paritioned on col(s): ${Partitions.getParquetFilePartitionCols(yellowTaxiParquetPath)}\")*/\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File: Delta"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//TODO - No support for delta - May be read the _delta_log json and checck for partition metadata in Delta 1.x and use Delta 2.0 API\n",
        "/*logDebug(log, s\"\\nPartition info for $yellowTaxiDeltaPath\")\n",
        "logDebug(log, s\"Is paritioned: ${Partitions.isDeltaFilePartitioned(yellowTaxiDeltaPath)}\")\n",
        "logDebug(log, s\"Paritioned on col(s): ${Partitions.getDeltaFilePartitionCols(yellowTaxiDeltaPath)}\")*/"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NewYorkTaxiYellow.write(\"/poc/delta/nyc_yellow_taxi_trips\", StorageFormat.Delta)\n",
        "NewYorkTaxiYellow.write(\"/poc/parquet/nyc_yellow_taxi_trips\", StorageFormat.Parquet)\n",
        "NewYorkTaxiYellow.write(\"/poc/csv/nyc_yellow_taxi_trips\", StorageFormat.Csv)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val nycTaxiParquetDf = spark.read.parquet(\"abfss://tlfs@xxx.dfs.core.windows.net/poc/parquet/nyc_yellow_taxi_trips/\")\n",
        "logDebug(log, \"Total rows: \"+nycTaxiParquetDf.count)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val nycTaxiDeltaDf = spark.read.format(\"delta\").load(\"abfss://tlfs@xxx.dfs.core.windows.net/poc/delta/nyc_yellow_taxi_trips/\")\n",
        "logDebug(log, \"Total rows: \"+nycTaxiDeltaDf.count)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "select count(*) from test.emp_table where ldt >= '2023-02-26' and ldt < '2023-02-27';\n",
        "select count(*) from test.emp_table where ldt >= '2023-02-26-00' and ldt < '2023-02-26-23';"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Round-robin\n",
        "\n",
        "`repartition(numPartitions: Int)` used RoundRobinParitioning."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hash"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Range"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce partitions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partition Management\n",
        "\n",
        "adding, deleting, and relocating specific partitions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partition Pruning"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Static"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [Why partitioning is needed and internals](https://medium.com/@vladimir.prus/spark-partitioning-the-fine-print-5ee02e7cb40b)\n",
        "- [Understanding Paritioning in Spark](https://sparkbyexamples.com/spark/spark-partitioning-understanding/)\n",
        "- [Scalable Partition Handling in Cloud-Native Spark Architectures](https://databricks.com/blog/2016/12/15/scalable-partition-handling-for-cloud-native-architecture-in-apache-spark-2-1.html)\n",
        "- [Catalog based partition handling of DataSource Tables](http://www.gatorsmile.io/spark-2-1-catalog-based-partition-handling-for-data-source-tables/)\n",
        "- [Dynamic Partition Pruning](https://www.youtube.com/watch?v=ME1KCAYO44o)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}