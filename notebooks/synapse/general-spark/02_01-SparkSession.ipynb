{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Configure SparkSession using magic command: %%configure\n",
        "\n",
        "In `02-SparkSession` notebook, you have got a general understanding of context vs session and setting session level properties \n",
        "programatically through `SparkConf.set(...)`. Synapse notebooks provides another simpler way i.e. the `%%configure` magic \n",
        "command. Spark specific properties shoild be set in `conf` element of the JSON body."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Before setting properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Session id: \"+ spark)\n",
        "println()\n",
        "\n",
        "println(\"Driver cores: \"+ spark.conf.get(\"spark.driver.cores\"))\n",
        "println(\"Driver memory: \"+ spark.conf.get(\"spark.driver.memory\"))\n",
        "println(\"Executor cores: \"+ spark.conf.get(\"spark.executor.cores\"))\n",
        "println(\"Executor memory: \"+ spark.conf.get(\"spark.executor.memory\"))\n",
        "println(\"Executor instances: \"+ spark.conf.get(\"spark.executor.instances\"))\n",
        "\n",
        "println()\n",
        "println(\"Driver max result size: \"+ spark.conf.get(\"spark.driver.maxResultSize\"))\n",
        "println(\"Livy num rows: \"+ spark.conf.getOption(\"livy.rsc.sql.num-rows\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Request 5 executor instances\n",
        "\n",
        "In the following section, you are\n",
        "\n",
        "- requesting 5 executors (More than the minimum executors defined in the pool definition) through spark property `numExecutors`\n",
        "- setting max results size to 10g through standard spark property `spark.driver.maxResultSize`\n",
        "- setting num of rows returned by configuring Livy property `livy.rsc.sql.num-rows`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{ //-f option forces a restart of the session\n",
        "    //You can get a list of valid parameters to config the session from https://github.com/cloudera/livy#request-body.\n",
        "    \"executorCores\":2, \n",
        "    \"numExecutors\":5,\n",
        "\n",
        "    \"conf\":{\n",
        "    //Example of standard spark property, to find more available properties please visit:https://spark.apache.org/docs/latest/configuration.html#application-properties.\n",
        "        \"spark.driver.maxResultSize\":\"10g\",\n",
        "    //Example of customized property, you can specify count of lines that Spark SQL returns by configuring \"livy.rsc.sql.num-rows\".\n",
        "        \"livy.rsc.sql.num-rows\":\"3000\" \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## After setting properties\n",
        "\n",
        "Note creation of the new session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Session id: \"+ spark)\n",
        "println()\n",
        "\n",
        "println(\"Driver cores: \"+ spark.conf.get(\"spark.driver.cores\"))\n",
        "println(\"Driver memory: \"+ spark.conf.get(\"spark.driver.memory\"))\n",
        "println(\"Executor cores: \"+ spark.conf.get(\"spark.executor.cores\"))\n",
        "println(\"Executor memory: \"+ spark.conf.get(\"spark.executor.memory\"))\n",
        "println(\"Executor instances: \"+ spark.conf.get(\"spark.executor.instances\"))\n",
        "\n",
        "println()\n",
        "println(\"Driver max result size: \"+ spark.conf.get(\"spark.driver.maxResultSize\"))\n",
        "println(\"Livy num rows: \"+ spark.conf.getOption(\"livy.rsc.sql.num-rows\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [Azure docs reference](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks#spark-session-configuration-magic-command)\n",
        "- [Spark SQL RESET & SET](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-conf-mgmt-reset.html)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}