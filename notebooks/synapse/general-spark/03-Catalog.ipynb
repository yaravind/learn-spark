{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Metastore and Catalog\n",
        "\n",
        "Azure Spark comes with Hive MetaStore (HMS) pre-configured. The HMS is shared across all the pools of the same Synapse workspace.\n",
        "You can leverage it to **define and manage the structure** on top of \n",
        "semi-structured data like file with the the helo of well know DBMS concepts of databases, tables, columns etc. In another words, you \n",
        "can use it define, persist metadata permanenly and manage its evolution. Persistence means the metadata survives cluster \n",
        "restarts, available across sessions and across different clusters.\n",
        "\n",
        "```\n",
        "1. SparkSession allows you to store subset of this metadata (createOrReplaceXXX() methods) but it is only available in that session scope \n",
        "and is lost after the session is closed.\n",
        "````\n",
        "\n",
        "When you want to persist the Hive catalog metadata outside of the workspace, and share catalog objects with other computational engines \n",
        "outside of the workspace, such as HDInsight and Azure Databricks, you can connect to an external Hive Metastore as explained in this [Use external Hive Metastore for Synapse Spark Pool article](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore). \n",
        "\n",
        "\n",
        "Metadata is (but not limited to) \n",
        "\n",
        "- databases\n",
        "- tables\n",
        "- views\n",
        "- schema\n",
        "    - columns names\n",
        "    - data types\n",
        "    - partitions\n",
        "    - comments\n",
        "    - Statistics (used by catalyst optimizer)\n",
        "\n",
        "You will explore each of these in the this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Catalog type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "println(\"Catalog type: \"+spark.conf.get(\"spark.sql.catalogImplementation\"))\n",
        "println(\"Metastore version: \"+spark.conf.get(\"spark.sql.hive.metastore.version\"))\n",
        "println()\n",
        "println(\"Default impl: \"+spark.sharedState.externalCatalog)\n",
        "println(\"Default impl delegate: \"+spark.sharedState.externalCatalog.unwrapped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data Warehouse location\n",
        "\n",
        "Synapse stores the data in the `warehouse` folder of the storage account created as part of the Synapse workspace creation. \n",
        "You can get the absolute path using the `spark.sql.warehouse.dir` property.\n",
        "\n",
        "- A database is represented by a folder with `.db` suffix at the end\n",
        "- A table is represented by a folder\n",
        "- Actual data is stored in files with the format (Parquet, Delta, CSV etc.) the user has chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(spark.conf.get(\"spark.sql.warehouse.dir\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Metastore database\n",
        "\n",
        "As you have seen above, Synapse uses Hive metastore to store metadata. The underlying RDBMS is Azure SQL. \n",
        "It can be inferred from the following properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.conf\n",
        "    .getAll\n",
        "    .filter{case (k,v)=> k.contains(\"spark.hadoop.javax.jdo\")}\n",
        "    .foreach{ case(k, v) => println(k+\" = \"+v)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## What is spark.sql.catalog.spark_catalog?\n",
        "\n",
        "This has to do with different implementations of different versions of the API and not important. Included here for completeness. \n",
        "Refer https://www.waitingforcode.com/apache-spark-sql/pluggable-catalog-api/read for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Default catalog: \"+spark.conf.get(\"spark.sql.defaultCatalog\"))\n",
        "println(spark.conf.get(\"spark.sql.catalog.spark_catalog\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Explore the catalog\n",
        "\n",
        "You have 2 choices to interact with the catalog \n",
        "\n",
        "1. `spark.catalog` object\n",
        "2. Spark SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### spark.catalog object\n",
        "\n",
        "The `org.apache.spark.sql.catalog.Catalog` instance, accessible through `spark.catalog`, is used to interact with the metastore programatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "val databasesDf=spark.catalog.listDatabases\n",
        "display(databasesDf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Grab the database location from above and explore the underlying folder structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%fs ls abfss://tlfs@syncdldev01.dfs.core.windows.net/synapse/workspaces/syn-tccc-cdl-use2-dev-01/warehouse/vbc.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "{//Introduce a code block so that the variables are not printed in the output\n",
        "    import com.microsoft.spark.notebook.msutils.MSFileInfo\n",
        "\n",
        "    val tables : Array[MSFileInfo] = mssparkutils.fs.ls(\"/synapse/workspaces/syn-tccc-cdl-use2-dev-01/warehouse/vbc.db\")\n",
        "    val tableOne = tables(0)\n",
        "    println(\"Table name: \"+tableOne.name)\n",
        "    println(\"Is Directory: \"+tableOne.isDir)\n",
        "    println(\"Is file: \"+tableOne.isFile)\n",
        "\n",
        "    //List the files of the table\n",
        "    val tableDetails : Array[MSFileInfo] = mssparkutils.fs.ls(\"/synapse/workspaces/syn-tccc-cdl-use2-dev-01/warehouse/vbc.db/vbc_sum_view_mth_brd_pkg_gt_wky\")\n",
        "    tableDetails.foreach{ f=> println(\"\\tFile: \"+f.name) }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "val tablesDf = spark.catalog.listTables(dbName = \"vbc\")\n",
        "display(tablesDf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SHOW DATABASES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql \n",
        "SHOW TABLES FROM vbc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SHOW TABLE EXTENDED FROM vbc LIKE 'vbc_sum_view_mth_brd_pkg_gtt';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "--Clean up\n",
        "DROP DATABASE IF EXISTS test CASCADE;\n",
        "\n",
        "-- Recreate\n",
        "CREATE DATABASE IF NOT EXISTS test;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Tables and views can be virtual if needed allowing decopuling of data & meatadata. You can \n",
        "\n",
        "- Insert or save data to it (managed tables) or\n",
        "- Associate the path of an already existing file (External or unmanaged tables) `createExternalTable()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "//Create an emptry table without any data\n",
        "import org.apache.spark.sql.types.{IntegerType,StringType,StructType,StructField}\n",
        "val simpleSchema = StructType(Array(\n",
        "    StructField(\"id\", IntegerType, true),\n",
        "    StructField(\"firstname\",StringType,true)\n",
        "  ))\n",
        "\n",
        "val emptyTableDf = spark.catalog.createTable(tableName=\"test.empty_table\", source=\"parquet\", schema=simpleSchema, options=Map.empty[String, String])\n",
        "emptyTableDf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Synapse has [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) package pre-installed. \n",
        "You will use [NYC Green Taxi trip records](https://azure.microsoft.com/en-us/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/) \n",
        "data in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "// Load nyc green taxi trip records from azure open dataset\n",
        "val blob_account_name = \"azureopendatastorage\"\n",
        "\n",
        "val nyc_blob_container_name = \"nyctlc\"\n",
        "val nyc_blob_relative_path = \"green\"\n",
        "val nyc_blob_sas_token = \"\"\n",
        "\n",
        "val nyc_wasbs_path = f\"wasbs://$nyc_blob_container_name@$blob_account_name.blob.core.windows.net/$nyc_blob_relative_path\"\n",
        "spark.conf.set(f\"fs.azure.sas.$nyc_blob_container_name.$blob_account_name.blob.core.windows.net\",nyc_blob_sas_token)\n",
        "\n",
        "val nyc_tlc_df = spark.read.parquet(nyc_wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Num partitions: \"+nyc_tlc_df.rdd.getNumPartitions)\n",
        "println(\"Records in each partition (Partition#, Record count)\")\n",
        "\n",
        "nyc_tlc_df.rdd.mapPartitionsWithIndex{\n",
        "                  // 'index' represents the Partition No, 'iterator' to iterate through all elements in the partition\n",
        "                         (index, iterator) => {\n",
        "                           println(\"Called in Partition -> \" + index)\n",
        "                           val numElements = iterator.length\n",
        "                           // In a normal user case, we will do the\n",
        "                           // the initialization(ex : initializing database)\n",
        "                           // before iterating through each element\n",
        "                           Array((index, numElements)).iterator\n",
        "                        }\n",
        "        }\n",
        "        .collect\n",
        "        .foreach(x => println(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "nyc_tlc_df.printSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Managed Tables\n",
        "\n",
        "A managed table is a Spark SQL table for which Spark manages both the data and the metadata. A global managed table is available \n",
        "across all clusters. When you drop the table both data and metadata gets dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "//Save data to managed table\n",
        "nyc_tlc_df.write.saveAsTable(\"test.nyc_green_taxi_trips\")\n",
        "\n",
        "//The schema from dataframe is used to create metadata\n",
        "display(spark.sql(\"SHOW TABLE EXTENDED FROM test LIKE 'nyc_green_taxi_trips'\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### External a.k.a UnManaged Tables\n",
        "\n",
        "Spark manages the metadata, while you control the data location. As soon as you add `path` option in dataframe \n",
        "writer or `location` keyword to `CREATE TABLE` statement it will be treated as global external/unmanaged table. \n",
        "When you drop table only metadata gets dropped. A global unmanaged/external table is available across all clusters.\n",
        "\n",
        "External tables are created \n",
        "\n",
        "1. When you use EXTERNAL keyword and specify LOCATION or \n",
        "2. LOCATION alone as part of CREATE TABLE\n",
        "\n",
        "**Differences**\n",
        "\n",
        "- When you drop a Managed Table, it will delete metadata from metastore as well as data. However, when you drop External Table, \n",
        "only metadata will be dropped, not the data. Typically you use External Table when same dataset is processed by multiple frameworks \n",
        "such as Synapse Serverless, Synapse SQL Pool (as external table/view), Spark, Pandas etc.\n",
        "- You cannot run TRUNCATE TABLE command against External Tables.\n",
        "\n",
        "You will use the Diabetes open data set for external table test.\n",
        "\n",
        "```\n",
        "In general CREATE TABLE is creating a “pointer”, and you need to make sure it points to something existing. \n",
        "An exception is file source such as parquet, json. If you don’t specify the LOCATION, Spark will create a default table location for you.\n",
        "```\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "//Print the diabetes open dataset Blob path so that you can use `fs` magic to copy data to your synapse storage account\n",
        "val diab_blob_container_name = \"mlsamples\"\n",
        "val diab_blob_relative_path = \"diabetes\"\n",
        "val diab_blob_sas_token = \"\"\n",
        "\n",
        "val diab_wasbs_path = f\"wasbs://$diab_blob_container_name@$blob_account_name.blob.core.windows.net/$diab_blob_relative_path\"\n",
        "println(diab_wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "-- Create table with only 3 columns even though the parquet file has 11 columns\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS test.diabetes\n",
        "(\n",
        "    AGE\tbigint COMMENT 'Age of the person',\n",
        "    BMI double COMMENT 'Body mass index (weight in kg/(height in m)^2)',\n",
        "    BP double COMMENT 'Diastolic blood pressure (mm Hg)'\n",
        ")\n",
        "USING PARQUET\n",
        "LOCATION 'wasbs://mlsamples@azureopendatastorage.blob.core.windows.net/diabetes'\n",
        "COMMENT 'The Diabetes dataset has 442 samples with 10 features';\n",
        "\n",
        "-- Query metadata\n",
        "DESCRIBE FORMATTED test.diabetes;\n",
        "\n",
        "-- Query for data\n",
        "SELECT COUNT(*) FROM test.diabetes;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create external table from a dataframe.\n",
        "\n",
        "```\n",
        "dataframe.write.option('path', \"<your-storage-path>\").saveAsTable(\"my_table\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Views"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Permanent Views\n",
        "\n",
        "- The view definition is recorded in the underlying metastore. \n",
        "- You can only create permanent view on **global managed table or global unmanaged table**. \n",
        "- Not allowed to create a permanent view on top of any temporary views or dataframe. \n",
        "\n",
        "```\n",
        "Note: Permanent views are only available in SQL API — not available in dataframe API\n",
        "```\n",
        "\n",
        "Persist a dataframe as permanent view. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "-- Create a view on top of managed table\n",
        "CREATE VIEW IF NOT EXISTS\n",
        "    test.view_nyc_green_taxi_trips \n",
        "AS \n",
        "    SELECT * FROM test.nyc_green_taxi_trips;\n",
        "\n",
        "-- Query total row count from View\n",
        "SELECT COUNT(1) FROM test.view_nyc_green_taxi_trips;\n",
        "\n",
        "-- Query total row count from Table\n",
        "SELECT COUNT(1) FROM test.nyc_green_taxi_trips;\n",
        "\n",
        "-- Show the views\n",
        "SHOW VIEWS FROM test;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Temporary Views\n",
        "\n",
        "These are single or multiple session scoped. The meatadata will be lost as soon as the underlying sessions are closed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Local Temp Views\n",
        "\n",
        "Temportary views are used generally \n",
        "\n",
        "- to share data between differnt languages in a notebook\n",
        "- to support SQL on DataFrames. Using Spark SQL requires metadata (schema, partitions) to be available. We can use the approach\n",
        "mentioned under 'Tables` section above to create this metadata but it is unnecessary if the scope of usage is local to current\n",
        "processing. In this scenarion a temporary view is useful.\n",
        "\n",
        "You can use `df.createOrReplaceTempView` to create the temp view. A temporary view is \n",
        "\n",
        "- Spark session scoped. \n",
        "- A local view is not accessible from other notebooks in Synapse\n",
        "- Not registered in the metastore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Create\n",
        "\n",
        "Spark SQL syntax that can replace the following 2 Scala statements: \n",
        "\n",
        "`CREATE TEMP VIEW temp_view_green_taxi_trips AS SELECT * FROM test.nyc_green_taxi_trips;`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val nycGreenTaxiDf = spark.sql(\"SELECT * FROM test.nyc_green_taxi_trips\")\n",
        "\n",
        "//Create the view in \"default\" database. You can create temp views in any database but it will not be permanent\n",
        "nycGreenTaxiDf.createOrReplaceTempView(\"temp_view_green_taxi_trips\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(spark.table(\"temp_view_green_taxi_trips\").limit(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "SHOW VIEWS;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Global Temp Views\n",
        "\n",
        "- Spark `application` scoped\n",
        "- Are tied to a system preserved temporary database `global_temp`. \n",
        "- Can be shared across different spark sessions (or if using databricks notebooks, then shared across notebooks).\n",
        "- Not supported in Synapse Spark.\n",
        "\n",
        "You can use `df.createOrReplaceGlobalTempView(\"my_global_view\")` and can be accessed as `spark.read.table(\"global_temp.my_global_view\")`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Create"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "nycGreenTaxiDf.createOrReplaceGlobalTempView(\"global_temp_view_green_taxi_trips\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "val globalTempViewTaxiDf = spark.read.table(\"global_temp.global_temp_view_green_taxi_trips\")\n",
        "display(globalTempViewTaxiDf.limit(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "-- List all views in global temp view database. \n",
        "-- Note that the command also lists local temporary views regardless of a given database.\n",
        "SHOW VIEWS FROM global_temp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Partitions\n",
        "\n",
        "You will use NYC Yello Taxi (1.5B rows, 50 GB) to demonstrate the Partitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Managed Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create a paritioned \"Managed\" table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "// Azure storage access info\n",
        "val blob_account_name = \"azureopendatastorage\"\n",
        "val blob_container_name = \"nyctlc\"\n",
        "val blob_relative_path = \"yellow\"\n",
        "val blob_sas_token = \"r\"\n",
        "\n",
        "// Allow SPARK to read from Blob remotely\n",
        "val wasbs_path = f\"wasbs://$blob_container_name@$blob_account_name.blob.core.windows.net/$blob_relative_path\"\n",
        "spark.conf.set(f\"fs.azure.sas.$blob_container_name.$blob_account_name.blob.core.windows.net\",blob_sas_token)\n",
        "println(\"Remote blob path: \" + wasbs_path)\n",
        "\n",
        "// SPARK read parquet, note that it won't load any data yet by now\n",
        "val nycYellowTaxiDf = spark.read.parquet(wasbs_path)\n",
        "\n",
        "println(\"Num partitions: \"+nycYellowTaxiDf.rdd.getNumPartitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "// Save as managed table\n",
        "nycYellowTaxiDf.write.partitionBy(\"puYear\", \"puMonth\").saveAsTable(\"test.nyc_yellow_taxi_trips_partitioned\")\n",
        "\n",
        "//The schema from dataframe is used to create metadata\n",
        "display(spark.sql(\"SHOW TABLE EXTENDED FROM test LIKE 'test.nyc_yellow_taxi_trips_partitioned'\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Check meatadata for partition info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "DESCRIBE TABLE EXTENDED test.nyc_yellow_taxi_trips_partitioned;\n",
        "\n",
        "-- Show all paritions\n",
        "SHOW PARTITIONS test.nyc_yellow_taxi_trips_partitioned;\n",
        "\n",
        "--Show details on specific partition\n",
        "SHOW PARTITIONS test.nyc_yellow_taxi_trips_partitioned PARTITION (puYear=\"2012\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### External a.k.a UnManaged Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create partitioned dataset on storage account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "// Repartition and write the partitions to ADLS under specified folder\n",
        "var pathExists = true;\n",
        "\n",
        "try{\n",
        "    mssparkutils.fs.ls(\"/poc/nyc_yellow_taxi_trips_partitioned\")\n",
        "} catch {\n",
        "    case ex: Exception => {pathExists = false}\n",
        "}\n",
        "\n",
        "if(!pathExists) {\n",
        "    nycYellowTaxiDf.write.partitionBy(\"puYear\", \"puMonth\").parquet(\"/poc/nyc_yellow_taxi_trips_partitioned\")\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Verify that the data is written to partitions and that we can read it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.read\n",
        "    .parquet(\"/poc/nyc_yellow_taxi_trips_partitioned\").limit(5)\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create External Table\n",
        "Add meta information to catalog by creating a table with \"path\" i.e. external or unmanaged table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "//Create external table\n",
        "spark.catalog.createTable(\"test.external_nyc_yellow_taxi_trips_partitioned\",\n",
        "                path=\"/poc/nyc_yellow_taxi_trips_partitioned\",\n",
        "                source=\"parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Check metadata for partition info\n",
        "The `DESCRIBE` commands output shows us the Partiotion columns have been identified by and persisted in the catalog but the `SELECT` \n",
        "queries still doesn't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "DESCRIBE TABLE EXTENDED test.external_nyc_yellow_taxi_trips_partitioned;\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "SELECT * FROM test.external_nyc_yellow_taxi_trips_partitioned;\n",
        "SHOW PARTITIONS test.external_nyc_yellow_taxi_trips_partitioned;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Recover (Fix) partition details into catalog\n",
        "As you can see from the above output `No data available`, the data in the path `/poc/nyc_yellow_taxi_trips_partitioned` is visible to \n",
        "file based readers but invisible while reading through the catalog using **Spark SQL**. If the external table is not partitioned, this is \n",
        "not an issue. but for partitioned datasets, this doesn't work unless we do a `recover` operation. \n",
        "\n",
        "```\n",
        "We can recover partitions by running MSCK REPAIR TABLE using spark.sql or by invoking spark.catalog.recoverPartitions.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "// When we use createTable to create partitioned table, we have to recover partitions so that partitions are visible.\n",
        "spark.catalog.recoverPartitions(\"test.external_nyc_yellow_taxi_trips_partitioned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Spark SQL Command can be used for recovering partitions as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "MSCK REPAIR TABLE test.external_nyc_yellow_taxi_trips_partitioned;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "SELECT * FROM test.external_nyc_yellow_taxi_trips_partitioned LIMIT 2;\n",
        "show PARTITIONS test.external_nyc_yellow_taxi_trips_partitioned;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "ANALYZE TABLE test.external_nyc_yellow_taxi_trips_partitioned COMPUTE STATISTICS;\n",
        "DESCRIBE EXTENDED test.external_nyc_yellow_taxi_trips_partitioned;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Statistics\n",
        "\n",
        "Statistics are supported for the following only:\n",
        "\n",
        "Hive Metastore tables for which ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan has been executed\n",
        "File-based data source tables for which the statistics are computed directly on the files of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Check stats using `explain(...)`\n",
        "\n",
        "Since Spark 3.0 you can use the `explain()` method to display the stats and see it not only for the table but for the actual query \n",
        "that we want to run. This can be done by using the new `mode` argument of the explain function. This is going to show us two query plans, \n",
        "namely the physical plan and also the optimized logical plan. The logical plan now contains the information about the statistics as \n",
        "you can see in the output of following command (Look for `Statistics` attribute under `Optimized Logical Plan`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.table(\"vbc.vbc_sum_view_mth_brd_pkg_gtt\").explain(mode=\"cost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Compute and check stats using Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "ANALYZE TABLE test.nyc_green_taxi_trips COMPUTE STATISTICS;\n",
        "DESCRIBE EXTENDED test.nyc_green_taxi_trips;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [Types of Apache Spark tables and views](https://medium.com/@subashsivaji/types-of-apache-spark-tables-and-views-f468e2e53af2)\n",
        "- [Spark Statistics Explained](https://towardsdatascience.com/statistics-in-spark-sql-explained-22ec389bf71b)\n",
        "- [Spark DDL SQL Reference](https://spark.apache.org/docs/latest/sql-ref-syntax.html)\n",
        "- [CREATE TABLE syntax](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-table.html)\n",
        "- [SHOW VIEWS syntax](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-show-views.html)\n",
        "- [Azure Open Datasets](https://docs.microsoft.com/en-us/azure/open-datasets/)\n",
        "- [Microsoft Research Open Data](https://msropendata.com/)"
      ]
    }
  ],
  "metadata": {
    "description": "AKA Metastores",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}