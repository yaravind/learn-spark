{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Spark Session in Azure Spark Notebook\n",
        "\n",
        "This notebook explains and demonstrates how **Spark Pool, Spark Instance** and **SparkSession**. \n",
        "Spark session gives a unified view of all the contexts and isolation of configuration and environment.\n",
        "\n",
        "**`Each notebook gets its own separate SparkSession, SparkContext and SQLContext`**\n",
        "\n",
        "The default spark session is already created for us and made available in notebook with the variable name `spark`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Default session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Spark Session 1 from variable spark: \"+ spark)\n",
        "val defaultSession = spark\n",
        "\n",
        "// SparkSession encapsulates spark context, hive context, SQL context\n",
        "println(\"SparkContext from spark session 1: \"+spark.sparkContext)\n",
        "println(\"SQLContext from spark session 1: \"+spark.sqlContext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The default SparkContext is also made available for you with the variable name `sc`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"SparkContext from variable sc: \"+sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create a NEW session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val session2 = spark.newSession()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"NEW Spark Session 2: \"+ session2)\n",
        "\n",
        "println(\"SparkContext from spark session 2: \"+session2.sparkContext)\n",
        "println(\"SQLContext from spark session 2: \"+session2.sqlContext)\n",
        "\n",
        "// Observe that the SparkContext is still the same as old one\n",
        "assert(defaultSession.sparkContext == session2.sparkContext)\n",
        "\n",
        "// SQLContexts might differ\n",
        "assert(defaultSession.sqlContext != session2.sqlContext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Demo - Session Scoping & Isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "val data = Seq(\n",
        "                (1, \"Aravind\"),\n",
        "                (2, \"Triveni\"),\n",
        "                (3, \"Esha\"),\n",
        "                (4, \"Rishik\")\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Session level views"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Since this DF was created using defaultSession, creating a view on this DF is only visible in that session's scope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "val familyDf1 = defaultSession.createDataFrame(defaultSession.sparkContext.parallelize(data))\n",
        "familyDf1.show()\n",
        "\n",
        "familyDf1.createOrReplaceTempView(\"family_tbl\")\n",
        "\n",
        "println(\"Visible in 'defaultSession' scope\")\n",
        "defaultSession.sql(\"show tables like 'family*'\").show()\n",
        "\n",
        "println(\"NOT visible in 'session2' scope\")\n",
        "session2.sql(\"show tables like 'family*'\").show()\n",
        "//display(session2.sql(\"show tables\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "If you want to have the same data available in session 2 then you need to create a new dataframe using session2. Once created you can register with the same name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val familyDf2 = session2.createDataFrame(session2.sparkContext.parallelize(data))\n",
        "familyDf2.show()\n",
        "\n",
        "familyDf2.createOrReplaceTempView(\"family_tbl\")\n",
        "\n",
        "println(\"Visible in 'session2' scope\")\n",
        "defaultSession.sql(\"show tables like 'family*'\").show()\n",
        "\n",
        "println(\"'defaultSession' also has the same DF that was registered earlier under the same name\")\n",
        "session2.sql(\"show tables like 'family*'\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Demo - Session level configuration\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Value of 'spark.sql.crossJoin.enabled' prop on both sessions\")\n",
        "println(\"defaultSession: \"+defaultSession.conf.get(\"spark.sql.crossJoin.enabled\"))\n",
        "println(\"session2: \"+session2.conf.get(\"spark.sql.crossJoin.enabled\"))\n",
        "\n",
        "println(\"\\nSet it to false on session 2\")\n",
        "session2.conf.set(\"spark.sql.crossJoin.enabled\", false)\n",
        "\n",
        "println(\"\\nAfter setting it to false on session 2\")\n",
        "println(\"defaultSession: \"+defaultSession.conf.get(\"spark.sql.crossJoin.enabled\"))\n",
        "println(\"session2: \"+session2.conf.get(\"spark.sql.crossJoin.enabled\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Demo - Close NEW session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "session2.close"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Spark SQL Session Mgmt commands\n",
        "\n",
        "- RESET\n",
        "- SET: The SET command sets a property, returns the value of an existing property or returns all \n",
        "SQLConf properties with value and meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SET spark.sql.crossJoin.enabled;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SET spark.sql.crossJoin.enabled=false;\n",
        "SET spark.sql.crossJoin.enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Which session is currently active?\n",
        "\n",
        "This method first checks whether there is a valid thread-local SparkSession, and if yes, return that one. \n",
        "**It then checks whether there is a valid global default SparkSession, and if yes, return that one.** If no valid global \n",
        "default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the \n",
        "global default. In case an existing SparkSession is returned, the config options specified in this builder will be applied \n",
        "to the existing SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import org.apache.spark.sql.SparkSession\n",
        "assert(defaultSession ==SparkSession.builder.getOrCreate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## What happens when you run the first cell of the notebook?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "When you run the first cell of a notebook a new `SparkSession` object is instantiated. That object can be referenced in the notebook with name: `spark`. \n",
        "This object is our entry point to the running spark jobs. Behind the scenes, \n",
        "\n",
        "1. Notebook submits the code to Livy job server\n",
        "2. Livy creates a new Livy ID representing the `SparkSession` for the current notebook\n",
        "3. Livy creates a new **Spark Instance** based on the **Spark Pool** definition provided by the pool the notebook is attached to (Livy uses YARN RM)\n",
        "4. Livy creates a new *app tag* with the conventions `<notebook-name>_<spark-pool-name>_<unix-epoch-timestamp>` and saves it in Zoo Keeper (this step is for resiliency). \n",
        "This app tag is passed to YARN as value to proprty `spark.yarn.tags`\n",
        "5. Livy submits the job to YARN which creates a *app id* and returns to Livy\n",
        "6. Livy associates the *app tag* (from step 4) with *app id* (from step 5)\n",
        "7. Livy returns the result of the job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [Synapse Job Server Architecture](https://github.com/yaravind/technical-reference/blob/d280f647f7bbaafaefbbb9f90626da3eac30b1fc/src/spark/Synapse%20Job%20Service.png)\n",
        "- [Synapse Spark Livy Endpoint Reference](https://docs.microsoft.com/en-us/rest/api/synapse/data-plane/spark-session)\n",
        "- [Spark Pool](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-concepts#spark-pools)\n",
        "- [Spark Instance](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-concepts#spark-instances)\n",
        "- [Best practices for Spark memory management](https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/)\n",
        "- [Spark SQL Session commands](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-conf-mgmt-reset.html)"
      ]
    }
  ],
  "metadata": {
    "description": "Demonstrates the relation between session  isolation & scope and sessions' relation to other SQL and Spark contexts.",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}