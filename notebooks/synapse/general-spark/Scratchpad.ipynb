{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure\n",
        "{\n",
        "    // You can get a list of valid parameters to config the session from https://github.com/cloudera/livy#request-body.\n",
        "    \"driverMemory\": \"4g\", // Recommended values: [\"28g\", \"56g\", \"112g\", \"224g\", \"400g\", \"472g\"]\n",
        "    \"driverCores\": 2, // Recommended values: [4, 8, 16, 32, 64, 80]\n",
        "    \"executorMemory\": \"4g\",\n",
        "    \"executorCores\": 2,\n",
        "    \"numExecutors\": 1,\n",
        "    \"conf\":\n",
        "    {\n",
        "        // Example of standard spark property, to find more available properties please visit: https://spark.apache.org/docs/latest/configuration.html#application-properties.\n",
        "        \"spark.driver.maxResultSize\": \"10g\",\n",
        "        // Example of customized property, you can specify count of lines that Spark SQL returns by configuring \"livy.rsc.sql.num-rows\".\n",
        "        \"livy.rsc.sql.num-rows\": \"3000\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "sc.version\n",
        "System.getProperty(\"java.version\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import com.tccc.dna.synapse.spark._\n",
        "\n",
        "SynapseSpark.printVersions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import java.lang.ClassLoader\n",
        "import java.net.URL\n",
        "\n",
        "val cl = ClassLoader.getSystemClassLoader\n",
        "val classpath: Array[URL] = cl.asInstanceOf[java.net.URLClassLoader].getURLs()\n",
        "\n",
        "println(s\"Total system classpath entries: ${classpath.size}\\n\")\n",
        "\n",
        "val connectors = classpath.filter(_.toString.contains(\"connector\"))\n",
        "connectors.foreach{println}\n",
        "println(s\"\\nTotal connector dependencies: ${connectors.size} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "import scala.sys.process._\n",
        "\"hdfs dfs -copyFromLocal /usr/hdp/5.0-67217682/spark3/jars/sqlanalyticsconnector-3.2.0-2.0.6.jar /poc\"!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "import scala.sys.process._\n",
        "\"hdfs dfs -copyFromLocal /usr/hdp/5.0-65587486/spark3/jars/sqlanalyticsconnector-3.2.0-2.0.6.jar /\"!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import scala.collection.immutable.Map\n",
        "val conf = spark.conf.getAll //returns Map[String, String]\n",
        "conf.foreach {keyVal => println(keyVal._1 + \"=\" + keyVal._2)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# List all \"line-level\" and \"cell-level\" magic commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%lsmagic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get help on \"line-level\" magic command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%help fs ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get list of spark jars (3 options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Option 1 - Use \"fs\" magic command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%fs ls /usr/hdp/5.0-62565507/spark3/jars/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Option 2 - Invoke hdfs commands using scala process class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "import scala.sys.process._\n",
        "\"hdfs dfs -ls file:///\"!!\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "import scala.sys.process._\n",
        "\"hdfs dfs -lsr file:///usr/hdp/\"!!\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Option 3 - Use \"mssparkutils\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "mssparkutils.fs.ls(\"/usr/hdp/5.0-65587486/spark3/jars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Use HDFS FileSystem API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import org.apache.hadoop.fs.{FileSystem, Path}\n",
        "FileSystem.get(sc.hadoopConfiguration).listFiles(new Path(\"/usr/hdp/5.0-59337622/spark3/jars\"), true)\n",
        "\n",
        "//java.nio.file.AccessDeniedException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", \n",
        "//403, \n",
        "//GET, https://syncdldev01.dfs.core.windows.net/tlfs?upn=false&resource=filesystem&maxResults=5000&directory=usr/hdp/5.0-59337622/spark3/jars&timeout=90&recursive=false, \n",
        "//AuthorizationPermissionMismatch, \"This request is not authorized to perform this operation using this permission. \n",
        "//RequestId:d26684f2-201f-0042-437e-a0fed1000000 Time:2022-07-25T23:31:56.2087623Z\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Copy Microsost & Azure dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "import scala.sys.process._\n",
        "\"hdfs dfs -copyFromLocal /usr/hdp/5.0-59337622/spark3/jars/sqlanalyticsconnector-3.1.2-2.0.5.jar /\"!!"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "scala"
    }
  }
}