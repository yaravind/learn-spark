{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Spark Configuration\n",
        "\n",
        "3 locations to configure the system\n",
        "\n",
        "1. Spark Properties\n",
        "    - Programatically by creating `org.apache.spark.SparkConf` or through Java System Properties. \n",
        "    Properties directly set on `SparkConf` take precedence over Java System Properties.\n",
        "    - We can set arbitrary (our own) properties using `set(...)`\n",
        "2. Environment variables\n",
        "    - Per machine settings\n",
        "    - Set through the `conf/spark-env.sh` script on each node\n",
        "3. Logging\n",
        "    - Configured through log4j2.properties\n",
        "\n",
        "**Note**\n",
        "```\n",
        "Synapse Spark uses YARN in cluster mode. For YARN environment variables need to be set using the \n",
        "spark.yarn.appMasterEnv.[EnvironmentVariableName] property in your conf/spark-defaults.conf file. \n",
        "Environment variables that are set in spark-env.sh will not be reflected in the YARN Application Master process in cluster mode. \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## View version information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import com.tccc.dna.synapse.spark.SynapseSpark\n",
        "SynapseSpark.printVersions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## View App information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "SynapseSpark.printAppInfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Viewing Spark Properties\n",
        "\n",
        "- Can be viewed through \n",
        "    1. From the **Environment tab** of the submitted application web UI at http://<driver>:4040 or \n",
        "    2. Programatically through `SparkConf.getXXX(...)` methods. \n",
        "- Note that only values **explicitly** specified through `spark-defaults.conf`, `SparkConf`, or the `command line` will appear.\n",
        " For all other configuration properties, you can assume the default value is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Viewing programatically\n",
        "\n",
        "You can retrieve the properties either through SparkSession or SparkContext. They both contain the same properties. `com.tccc.dna.synapse.spark.SynapseSpark` encapsulates the required logic to view all configured properties. This notebock will illustrate its usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "{\n",
        "    import org.apache.spark.SparkConf\n",
        "\n",
        "    val confThruSession = spark.conf.getAll\n",
        "    val confThruContext = sc.getConf.getAll\n",
        "\n",
        "    println(\"Conf HashCode from SparkSession/RuntimeConfig:\"+ confThruSession.hashCode)\n",
        "    println(\"Conf HashCode from SparkContext: \"+ confThruContext.hashCode)\n",
        "\n",
        "    println(\"Conf size from SparkSession/RuntimeConfig:\"+ confThruSession.size)\n",
        "    println(\"Conf size from SparkContext: \"+ confThruContext.size)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Get properties from SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printAppInfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Get properties from SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"App Name (from context): \"+ sc.appName)\n",
        "println(\"App Id (from context): \"+ sc.applicationId)\n",
        "SynapseSpark.printAppInfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Explicitly accessed props\n",
        "\n",
        "***\n",
        "```\n",
        "Note that only values explicitly specified through spark-defaults.conf, SparkConf, or the command line will appear. \n",
        "For all other configuration properties, you can assume the default value is used or no value is set or set through \n",
        "other configuration locations.\n",
        "```\n",
        "***\n",
        "\n",
        "*spark.sql.sources.default* \n",
        "\n",
        "- Default: parquet\n",
        "- Used when:\n",
        "    - Reading (DataFrameWriter) or writing (DataFrameReader) datasets\n",
        "    - Creating external table from a path (in Catalog.createExternalTable)\n",
        "    - Reading (DataStreamReader) or writing (DataStreamWriter) in Structured Streaming\n",
        "\n",
        "Reference: https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-properties.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"--AQE\")\n",
        "println(\"spark.sql.adaptive.enabled (When true, re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics) = \"+spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
        "println(\"spark.sql.adaptive.forceOptimizeSkewedJoin = \"+spark.conf.getOption(\"spark.sql.adaptive.forceOptimizeSkewedJoin\"))\n",
        "\n",
        "println(\"\\n--CSV\")\n",
        "println(\"spark.sql.csv.filterPushdown.enabled (When true, enable filter pushdown to CSV datasource.) = \"+spark.conf.get(\"spark.sql.csv.filterPushdown.enabled\"))\n",
        "\n",
        "println(\"\\n--JSON\")\n",
        "println(\"spark.sql.json.filterPushdown.enabled = \"+spark.conf.get(\"spark.sql.json.filterPushdown.enabled\"))\n",
        "\n",
        "println(\"\\n--Parquet\")\n",
        "println(\"spark.sql.parquet.aggregatePushdown = \"+spark.conf.getOption(\"spark.sql.parquet.aggregatePushdown\"))\n",
        "println(\"spark.sql.parquet.filterPushdown = \"+spark.conf.get(\"spark.sql.parquet.filterPushdown\"))\n",
        "println(\"spark.sql.parquet.outputTimestampType = \"+spark.conf.get(\"spark.sql.parquet.outputTimestampType\"))\n",
        "println(\"spark.sql.parquet.recordLevelFilter.enabled = \"+spark.conf.get(\"spark.sql.parquet.recordLevelFilter.enabled\"))\n",
        "\n",
        "println(\"\\n--SQL DML/DDL\")\n",
        "/*\n",
        "When true, Spark replaces CHAR type with VARCHAR type in CREATE/REPLACE/ALTER TABLE commands, so that newly created/updated tables will not have CHAR \n",
        "type columns/fields. Existing tables with CHAR type columns/fields are not affected by this config.\n",
        "*/\n",
        "println(\"spark.sql.charAsVarchar = \"+spark.conf.getOption(\"spark.sql.charAsVarchar\"))\n",
        "println(\"spark.sql.groupByAliases (When true, aliases in a select list can be used in group by clauses. When false, an analysis exception is thrown in the case.) = \"+spark.conf.get(\"spark.sql.groupByAliases\"))\n",
        "println(\"spark.sql.groupByOrdinal = \"+spark.conf.get(\"spark.sql.groupByOrdinal\"))\n",
        "\n",
        "println(\"\\n--Maven\")\n",
        "println(\"spark.sql.maven.additionalRemoteRepositories = \"+spark.conf.get(\"spark.sql.maven.additionalRemoteRepositories\"))\n",
        "\n",
        "//Thse are not set in Synapse spark\n",
        "println(\"\\n--Task\")\n",
        "println(\"spark.task.cpus (The number of CPU cores to schedule (allocate) to a task) = \"+spark.conf.getOption(\"spark.task.cpus\"))\n",
        "println(\"spark.task.maxFailures (Number of failures of a single task (of a TaskSet) before giving up on the entire TaskSet and then the job) = \"+spark.conf.getOption(\"spark.task.maxFailures\"))\n",
        "\n",
        "println(\"\\n--Memory\")\n",
        "println(\"spark.memory.fraction (Fraction of JVM heap space used for execution and storage) = \"+spark.conf.getOption(\"spark.memory.fraction\"))\n",
        "println(\"spark.memory.offHeap.enabled (Controls whether Tungsten memory will be allocated on the JVM heap (false) or off-heap) = \"+spark.conf.getOption(\"spark.memory.offHeap.enabled\"))\n",
        "println(\"spark.memory.offHeap.size (Maximum memory (in bytes) for off-heap memory allocation) = \"+spark.conf.getOption(\"spark.memory.offHeap.size\"))\n",
        "\n",
        "println(\"\\n--Shuffle\")\n",
        "println(\"spark.sql.shuffle.partitions = \"+spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
        "println(\"spark.shuffle.compress (Controls whether to compress shuffle output when stored) = \"+spark.conf.getOption(\"spark.shuffle.compress\"))\n",
        "println(\"spark.shuffle.manager = \"+spark.conf.getOption(\"spark.shuffle.manager\"))\n",
        "println(\"spark.plugins (A comma-separated list of class names implementing org.apache.spark.api.plugin.SparkPlugin to load into a Spark application.) = \"+spark.conf.getOption(\"spark.plugins\"))\n",
        "\n",
        "println(\"\\n--Driver\")\n",
        "println(\"spark.driver.log.dfsDir = \"+spark.conf.getOption(\"spark.driver.log.dfsDir\"))\n",
        "\n",
        "println(\"\\n--Other\")\n",
        "println(\"spark.sql.defaultCatalog = \"+spark.conf.get(\"spark.sql.defaultCatalog\"))\n",
        "println(\"spark.sql.ansi.enabled = \"+spark.conf.get(\"spark.sql.ansi.enabled\"))\n",
        "println(\"spark.sql.columnNameOfCorruptRecord (The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.) = \"+spark.conf.get(\"spark.sql.columnNameOfCorruptRecord\"))\n",
        "//Default number of partitions in resilient distributed datasets (RDDs) returned by transformations like join, reduceByKey, and parallelize when no partition number is set by the user.\n",
        "println(\"spark.default.parallelism (Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize) = \"+spark.conf.getOption(\"spark.default.parallelism\"))\n",
        "println(\"spark.sql.leafNodeDefaultParallelism = \"+spark.conf.get(\"spark.sql.leafNodeDefaultParallelism\"))\n",
        "println(\"spark.sql.sources.default = \"+spark.conf.get(\"spark.sql.sources.default\"))\n",
        "println(\"spark.sql.files.maxRecordsPerFile (Maximum number of records to write out to a single file. If this value is zero or negative, there is no limit.) = \"+spark.conf.get(\"spark.sql.files.maxRecordsPerFile\"))\n",
        "println(\"spark.sql.files.minPartitionNum = \"+spark.conf.get(\"spark.sql.files.minPartitionNum\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Master props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfMasterProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Driver props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfDriverProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Executor props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfExecutorProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Application props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfAppProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### YARN App props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfYARNAppProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### YARN props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfYARNProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Livy props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfLivyProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Spark SQL props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfSQLProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Azure props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfAzureProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Microsoft props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfMicrosoftProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Synapse props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfSynapseProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Spark UI props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfSparkUIProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Spark History Server props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfHistoryServerProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cluster props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfClusterProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Shuffle props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfShuffleProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Dynamic Allocation props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfDynamicAllocProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Synapse History Server conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfSynapseHistoryServerProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### External Hive Metastore props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfHiveMetastoreProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Event log props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfEventLogProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Data Locality Props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfLocalityProps()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Other props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfOtherProps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Delta props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfDeltaProps()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Databricks props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.printSparkConfDatabricksProps()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Unsupported properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "SynapseSpark.getSparkConfProp(\"spark.databricks.delta.schema.autoMerge.enabled\")\n",
        "SynapseSpark.getSparkConfProp(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\")\n",
        "SynapseSpark.getSparkConfProp(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [Spark Configuration documentation](https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties)\n",
        "- [External Metastore](https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore#set-up-an-external-metastore-using-the-ui)\n",
        "- [YARN properties](https://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties)"
      ]
    }
  ],
  "metadata": {
    "description": "Demonstrates default spark configurations of the underlying runtime",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "scala"
    }
  }
}