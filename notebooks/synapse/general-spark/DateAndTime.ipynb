{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Spark SQL defines the timestamp type as TIMESTAMP WITH SESSION TIME ZONE, which is a combination of the fields (YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, SESSION TZ) where the YEAR through SECOND field identify a time instant in the UTC time zone, and where SESSION TZ is taken from the SQL config spark.sql.session.timeZone. The session time zone can be set as:\n",
        "\n",
        "Zone offset '(+|-)HH:mm'. This form allows us to define a physical point in time unambiguously. Time zone name in the form of region ID 'area/city', such as 'America/Los_Angeles'. This form of time zone info suffers from some of the problems that we described above like overlapping of local timestamps. However, each UTC time instant is unambiguously associated with one time zone offset for any region ID, and as a result, each timestamp with a region ID based time zone can be unambiguously converted to a timestamp with a zone offset. By default, the session time zone is set to the default time zone of the Java virtual machine.\n",
        "\n",
        "Spark’s TIMESTAMP WITH SESSION TIME ZONE is different from:\n",
        "\n",
        "TIMESTAMP WITHOUT TIME ZONE, because a value of this type can map to multiple physical time instants, but any value of TIMESTAMP WITH SESSION TIME ZONE is a concrete physical time instant. The SQL type can be emulated by using one fixed time zone offset across all sessions, for instance UTC+0. In that case, we could consider timestamps at UTC as local timestamps. TIMESTAMP WITH TIME ZONE, because according to the SQL standard column values of the type can have different time zone offsets. That is not supported by Spark SQL.\n",
        "\n",
        "Spark SQL provides a few methods for constructing date and timestamp values:\n",
        "\n",
        "Default constructors without parameters: CURRENT_TIMESTAMP() and CURRENT_DATE().\n",
        "\n",
        "From other primitive Spark SQL types, such as INT, LONG, and STRING\n",
        "From external types like Python datetime or Java classes java.time.LocalDate/Instant.\n",
        "Deserialization from data sources CSV, JSON, Avro, Parquet, ORC or others.\n",
        "The function MAKE_DATE introduced in Spark 3.0 takes three parameters: YEAR, MONTH of the year, and DAY in the month and makes a DATE value. All input parameters are implicitly converted to the INT type whenever possible. The function checks that the resulting dates are valid dates in the Proleptic Gregorian calendar, otherwise it returns NULL. For example in PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(spark.conf.get(\"spark.sql.session.timeZone\"))\n",
        "\n",
        "/*\n",
        "There is NO difference between UTC and Etc/UTC time zones.\n",
        "\n",
        "Etc/UTC is a timezone in the Olson-timezone-database (tz database), also known as IANA-timezones-database, in which all timezones conform to a uniform naming convention: Area/Location.\n",
        "\n",
        "Since, some timezones cannot be attributed to any Area of the world (i.e. continents or oceans), the special Area Etc (Etcetera) was introduced. This applies mainly to administrative timezones such as UTC.\n",
        "Thus, to conform with the naming convention, the universal coordinated time(zone) is named Etc/UTC in the tz database.\n",
        "*/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Calendars\n",
        "\n",
        "- Lunar\n",
        "- Julian: Only used in history\n",
        "- Gregorian\n",
        "    - Introduced in 1582\n",
        "    - Used almost everywhere for civil purposes\n",
        "- Proleptic Gregorian\n",
        "    - Extension to Greorian calendar to support dates before 1582\n",
        "    - Default in Java 8, Pandas, R and Apache Arrow\n",
        "    - Default startomg Spark 3.0 (Before it used a combination of Julian (for dates before 1582) and Gregorian calendar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## TimeStamp\n",
        "\n",
        "- Extends `Date` type with `hour`, `minute`, `second` (with optional fractional part representing microseconds) and together with a global session scoped `time zone`\n",
        "- E.g. year=2012, month=12, day=31, hour=23, minute=59, second=59.123456, session timezone = UTC+01:00\n",
        "- When writing `Timestamp` to non-text sources like Parquet, the values are just `instants` with no time zone info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "- Instant: The Instant class represents an instant in time\n",
        "- LocalDate\n",
        "    - The “local” part of the name refers to the local time-line. Specifically, a LocalDate has no reference to a time-zone or offset from UTC/Greenwich.\n",
        "- LocalTime\n",
        "    - It is ideal for representing time on a wall clock such as 10:15:30. For example when we say “Time is ten past three”, it can be represented using ‘LocalTime’ as 3:10:00. LocalTime supports nanosecond precision. The minimum value for LocalTime is midnight (00:00:00.0) at start of the day and maximum value is one nanosecond before midnight(23:59:59.999999999) at end of day.\n",
        "- LocalDateTime\n",
        "- OffsetTime\n",
        "- ZonedDateTime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## TimeZone\n",
        "\n",
        "- `Date` type doesn't consider time zone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import java.time._\n",
        "java.time.ZoneId.systemDefault\n",
        "java.time.ZoneId.of(\"America/Los_Angeles\").getRules.getOffset(java.time.LocalDateTime.parse(\"1883-11-10T00:00:00\"))\n",
        "\n",
        "def getDateTimeWithTZOffset(dateTime:LocalDateTime, zoneStr:String): ZonedDateTime =\n",
        "{\n",
        "    ZonedDateTime.ofInstant(dateTime.toInstant(ZoneOffset.UTC), ZoneId.of(zoneStr))\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "val localTime = LocalTime.now()\n",
        "val localDateTime = LocalDateTime.now()\n",
        "\n",
        "println(s\"Local date and time: $localDateTime\")\n",
        "println(s\"Local date: ${localDateTime.toLocalDate}\")\n",
        "println(s\"Local time: ${localDateTime.toLocalTime}\")\n",
        "println(s\"System default timezone: ${java.time.ZoneId.systemDefault}\")\n",
        "\n",
        "println(\"Etc/UTC: \"+getDateTimeWithTZOffset(localDateTime, \"Etc/UTC\"))\n",
        "println(\"US/Eastern: \"+getDateTimeWithTZOffset(localDateTime, \"US/Eastern\"))\n",
        "println(\"US/Central: \"+getDateTimeWithTZOffset(localDateTime, \"US/Central\"))\n",
        "println(\"US/Mountain: \"+getDateTimeWithTZOffset(localDateTime, \"US/Mountain\"))\n",
        "println(\"US/Pacific: \"+getDateTimeWithTZOffset(localDateTime, \"US/Pacific\"))\n",
        "\n",
        "println(\"Etc/UTC: \"+getDateTimeWithTZOffset(localDateTime, \"Etc/UTC\").toLocalTime)\n",
        "println(\"US/Eastern: \"+getDateTimeWithTZOffset(localDateTime, \"US/Eastern\").toLocalTime)\n",
        "println(\"US/Central: \"+getDateTimeWithTZOffset(localDateTime, \"US/Central\").toLocalTime)\n",
        "println(\"US/Mountain: \"+getDateTimeWithTZOffset(localDateTime, \"US/Mountain\").toLocalTime)\n",
        "println(\"US/Pacific: \"+getDateTimeWithTZOffset(localDateTime, \"US/Pacific\").toLocalTime)\n",
        "\n",
        "println(\"Etc/UTC: \"+getDateTimeWithTZOffset(localDateTime, \"Etc/UTC\").toInstant().toEpochMilli())\n",
        "println(\"US/Eastern: \"+getDateTimeWithTZOffset(localDateTime, \"US/Eastern\").toInstant().toEpochMilli())\n",
        "println(\"US/Central: \"+getDateTimeWithTZOffset(localDateTime, \"US/Central\").toInstant().toEpochMilli())\n",
        "println(\"US/Mountain: \"+getDateTimeWithTZOffset(localDateTime, \"US/Mountain\").toInstant().toEpochMilli())\n",
        "println(\"US/Pacific: \"+getDateTimeWithTZOffset(localDateTime, \"US/Pacific\").toInstant().toEpochMilli())\n",
        "\n",
        "ZoneId.getAvailableZoneIds.stream.filter(x=>x.contains(\"US\") ).forEach(println)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Date and Time Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Creating Date and Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### CURRENT_DATE()/CURRENT_TIME()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT CURRENT_DATE(), CURRENT_TIMESTAMP()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### From external types: java.time.LocalDate/Instant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "val date = LocalDate.now\n",
        "val instant = Instant.now\n",
        "\n",
        "val currTimeInMillis = java.util.Calendar.getInstance().getTimeInMillis()\n",
        "val sqlDate = new java.sql.Date(currTimeInMillis)\n",
        "val sqlInstant = new java.sql.Timestamp(currTimeInMillis)\n",
        "// val legacyDate = new java.util.Date() //java.lang.UnsupportedOperationException: No Encoder found for java.util.Date\n",
        "//val time = LocalTime.now // fails with java.lang.UnsupportedOperationException: No Encoder found for java.time.LocalTime\n",
        "//val dateTime = LocalDateTime.now // java.lang.UnsupportedOperationException: No Encoder found for java.time.LocalDateTime\n",
        "\n",
        "import spark.implicits._\n",
        "val df1 = List(\n",
        "    (date, instant, sqlDate, sqlInstant)\n",
        ").toDF (\"java8_local_date\", \"java8_instant\", \"old_sql_date\", \"old_sql_instant\")\n",
        "df1.show(truncate=false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Reference\n",
        "\n",
        "- [Must read: A Comprehensive Look at Dates and Timestamps in Apache Spark™ 3.0](https://www.databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}