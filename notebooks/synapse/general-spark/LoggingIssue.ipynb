{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "1. TRACE: The TRACE Level designates finer-grained informational events than the DEBUG.\n",
        "2. DEBUG: The DEBUG Level designates fine-grained informational events that are most useful to debug an application.\n",
        "3. INFO: The INFO level designates informational messages that highlight the progress of the application at coarse-grained level.\n",
        "4. WARN: The WARN level designates potentially harmful situations.\n",
        "5. ERROR: The ERROR level designates error events that might still allow the application to continue running.\n",
        "6. FATAL: The FATAL level designates very severe error events that will presumably lead the application to abort.\n",
        "7. ALL: The ALL has the lowest possible rank and is intended to turn on all logging.\n",
        "8. OFF: The OFF has the highest possible rank and is intended to turn off logging.\n",
        "\n",
        "Rank\\Priority order: `ALL < TRACE < DEBUG < INFO < WARN < ERROR < FATAL < OFF`\n",
        "\n",
        "If we set level to `WARN` then only `warn`,  `ERROR`, and `FATAL` gets printed.\n",
        "\n",
        "```\n",
        "Synapse Spark configures default level to `INFO` so trace and debug can't be printed.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Demonstrace default behavior\n",
        "\n",
        "By default, the following are logged to Log Analytics:\n",
        "\n",
        "- INFO\n",
        "- WARN \n",
        "- ERROR\n",
        "- FATAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Log4j 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "//Log4j 1.x API\n",
        "val log = org.apache.log4j.LogManager.getLogger(s\"com.aravind.notebook.log4j.1x\")\n",
        "\n",
        "//sc.setLogLevel(\"DEBUG\")\n",
        "//Rank order ALL < TRACE < DEBUG < INFO < WARN < ERROR < FATAL < OFF\n",
        "\n",
        "log.trace(\"log4j.1x TRACE\")\n",
        "log.debug(\"log4j.1x DEBUG\")\n",
        "log.info(\"log4j.1x INFO\")\n",
        "log.warn(\"log4j.1x WARN\")\n",
        "log.error(\"log4j.1x ERROR\")\n",
        "log.fatal(\"log4j.1x FATAL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import com.tccc.dna.synapse.Logs._\n",
        "import com.tccc.dna.synapse.spark.SynapseSpark\n",
        "\n",
        "val notebookName = SynapseSpark.getCurrentNotebookName\n",
        "val aravindLog = org.apache.log4j.LogManager.getLogger(s\"com.aravind.notebook.$notebookName.log4j.1x\")\n",
        "//aravindLog.setLevel(org.apache.log4j.Level.DEBUG)\n",
        "logTrace(aravindLog, \"Logs api - Trace message\")\n",
        "logDebug(aravindLog, \"Logs api - Debug message\")\n",
        "logInfo(aravindLog, \"Logs api - Info message\")\n",
        "logWarning(aravindLog, \"Logs api - Warn message\")\n",
        "logError(aravindLog, \"Logs api - Error message\")\n",
        "logFatal(aravindLog, \"Logs api - Fatal message\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Log4j 2.x - Using LogManager\n",
        "\n",
        "`LogManager` is a static utility class that manages all loggers within an application. When you call this method, it checks if the requested logger already exists; if not, it creates a new one based on the log4j2 configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "//Log4j 2.x API\n",
        "import org.apache.logging.log4j.core._\n",
        "\n",
        "val rootLogger = org.apache.logging.log4j.LogManager.getLogger(s\"com.aravind.notebook.log4j.2x\")\n",
        "\n",
        "rootLogger.trace(\"log4j.2x root TRACE\")\n",
        "rootLogger.debug(\"log4j.2x root DEBUG\")\n",
        "rootLogger.info(\"log4j.2x root INFO\")\n",
        "rootLogger.warn(\"log4j.2x root WARN\")\n",
        "rootLogger.error(\"log4j.2x root ERROR\")\n",
        "rootLogger.fatal(\"log4j.2x root FATAL\")\n",
        "println(rootLogger.isTraceEnabled)\n",
        "println(rootLogger.isDebugEnabled)\n",
        "println(rootLogger.isInfoEnabled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Log4j 2.x - Using LoggerContext\n",
        "\n",
        "Use `LoggerContext` when you want to manipulate the logging configuration programmatically or load a custom configuration. In general, you will likely use `LogManager` more often since it's the primary entry point for logging in most applications. LoggerContext is used when you need advanced control over Log4j configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val cctx = LoggerContext.getContext()\n",
        "\n",
        "println(cctx.getName)\n",
        "\n",
        "val log2=cctx.getLogger(\"aravind.LoggerContext API\")\n",
        "log2.trace(\"log2 TRACE\")\n",
        "log2.debug(\"log2 DEBUG\")\n",
        "log2.info(\"log2 INFO\")\n",
        "log2.warn(\"log2 WARN\")\n",
        "log2.error(\"log2 ERROR\")\n",
        "log2.fatal(\"log2 FATAL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Exceptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "log.error(\"Exception without message\", new RuntimeException)\n",
        "log.error(\"Exception with message\", new RuntimeException(\"Something went wrong\"))\n",
        "log.fatal(\"Exception without message\", new IllegalStateException)\n",
        "log.fatal(\"Exception with message\", new IllegalStateException(\"Something went wrong\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "try {\n",
        "    throw  new NullPointerException(\"Test stack trace\")\n",
        "} catch  {\n",
        "    case ex: Exception => {\n",
        "        log.error(\"Exception Caguht nullpointer in try/catch\", ex)\n",
        "        log.fatal(\"Exception Caguht nullpointer in try/catch\", ex)\n",
        "        logError(aravindLog, \"Exception Caguht nullpointer in try/catch\", ex)\n",
        "        rootLogger.error(\"Exception Caguht nullpointer in try/catch\", ex)\n",
        "        rootLogger.fatal(\"Exception Caguht nullpointer in try/catch\", ex)\n",
        "        //log2.fatal(\"Exception Caguht nullpointer in try/catch\", ex)\n",
        "        //log2.error(\"Exception Caguht nullpointer in try/catch\", ex)\n",
        "        //log2.fatal(\"Exception Caguht nullpointer in try/catch\", ex)\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Exception in Worker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import org.apache.spark.sql.Row\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val numRdd = sc.parallelize(Seq(\n",
        "    Row(1,1), Row(1,0), Row(2,0)\n",
        "))\n",
        "\n",
        "val schema = StructType(Array(\n",
        "    StructField(\"a\",DoubleType,true),\n",
        "    StructField(\"b\",DoubleType,true)\n",
        "    ))\n",
        "\n",
        "val numDf = spark.createDataFrame(numRdd, schema)\n",
        "numDf.createOrReplaceTempView(\"number_tbl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Open issue with product team: \n",
        "\n",
        "When workers raise exceptions, the following columns are not populated. We expect these to be populated. These are populated when the exception is raised from driver though.\n",
        "exception_exception_class_s\n",
        "exception_exception_message_s\n",
        "exception_stacktrace_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        }
      },
      "source": [
        "%%sql\n",
        "SELECT a, b from number_tbl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##  Disable Log Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(log)\n",
        "println(aravindLog)\n",
        "//println(log2)\n",
        "\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "log.debug(\"after changing log levl to ERROR - DEBUG msg\")\n",
        "log.info(\"after changing log levl to ERROR - INFO msg\")\n",
        "log.warn(\"after changing log levl to ERROR - WARN msg\")\n",
        "log.error(\"after changing log levl to ERROR - ERROR msg\")\n",
        "log.fatal(\"after changing log levl to ERROR - FATAL msg\")\n",
        "\n",
        "aravindLog.info(\"after changing log levl to ERROR - DEBUG msg\")\n",
        "aravindLog.info(\"after changing log levl to ERROR - INFO msg\")\n",
        "aravindLog.info(\"after changing log levl to ERROR - WARN msg\")\n",
        "aravindLog.error(\"after changing log levl to ERROR - ERROR msg\")\n",
        "aravindLog.fatal(\"after changing log levl to ERROR - FATAL msg\")\n",
        "\n",
        "//Change it back\n",
        "sc.setLogLevel(\"INFO\")\n",
        "aravindLog.info(\"after changing log levl back to INFO - INFO msg\")\n",
        "aravindLog.info(\"after changing log levl back to INFO - WARN msg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**The root logger is set to `INFO`. We can change it to to `DEBUG`** as shown below but it is highly recommended to use this only if absolutely necessary. \n",
        "\n",
        "NOTE: Always log at INFO level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import org.apache.logging.log4j.LogManager\n",
        "import org.apache.logging.log4j.core.LoggerContext\n",
        "val ctx = LogManager.getContext(false).asInstanceOf[LoggerContext]\n",
        "val config = ctx.getConfiguration\n",
        "config.removeFilter(config.getFilter)\n",
        "\n",
        "\n",
        "sc.setLogLevel(\"DEBUG\")\n",
        "log.debug(\"after changing log level to DEBUG - DEBUG msg\")\n",
        "log.info(\"after changing log level to DEBUG - INFO msg\")\n",
        "\n",
        "sc.setLogLevel(\"INFO\")\n",
        "log.debug(\"after changing log level to INFO - DEBUG msg\")\n",
        "log.info(\"after changing log level to INFO - INFO msg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## MDC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import org.apache.log4j.MDC\n",
        "MDC.put(\"TestKey 1\", \"TestValue 1\")\n",
        "MDC.put(\"TestKey 2\", \"TestValue 2\")\n",
        "\n",
        "spark.sparkContext.setLocalProperty(\"mdc.\" + \"name\", \"value\")\n",
        "\n",
        "\n",
        "log.info(\"Test MDC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val driverLog4jPath = System.getProperty(\"log4j2.configurationFile\")\n",
        "val rootLogLevel = System.getProperty(\"log4jspark.root.logger\").split(\",\")(0)\n",
        "val log4jAppenders = System.getProperty(\"log4jspark.root.logger\").split(\",\").slice(1,2)\n",
        "val logDirPath = System.getProperty(\"log4jspark.log.dir\")\n",
        "val user = System.getProperty(\"user.name\")\n",
        "import scala.sys.process._\n",
        "//\"hdfs dfs -copyFromLocal \" + driverLog4jPath + \" /\"!!\n",
        "//\"hdfs dfs -copyFromLocal \" + log4jPatternPath + \" /\"!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val submitTime = System.getProperty(\"spark.app.submitTime\")\n",
        "val startTime = System.getProperty(\"spark.app.startTime\")"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}