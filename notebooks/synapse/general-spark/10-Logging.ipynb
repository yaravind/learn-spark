{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Application logs in Synapse Spark\n",
        "\n",
        "By default, Spark adds 1 record to the MDC (Mapped Diagnostic Context): mdc.taskName, which shows something like task 1.0 in stage 0.0. You can add %X{mdc.taskName} to your patternLayout in order to print it in the logs. Moreover, you can use spark.sparkContext.setLocalProperty(s\"mdc.$name\", \"value\") to add user specific data into MDC. The key in MDC will be the string of “mdc.$name”.\n",
        "\n",
        "https://spark.apache.org/docs/latest/configuration.html#configuring-logging\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-log-analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Log4J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Loggers/Cetoggories & Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import org.apache.log4j._\n",
        "import org.apache.log4j.spi._\n",
        "\n",
        "val logRepo:LoggerRepository = LogManager.getLoggerRepository\n",
        "\n",
        "import scala.collection.JavaConversions._\n",
        "val currentLoggers=logRepo.getCurrentLoggers().asInstanceOf[java.util.Enumeration[Logger]].toSet\n",
        "\n",
        "println(s\"Total loggers: ${currentLoggers.size}\")\n",
        "\n",
        "currentLoggers.foreach{\n",
        "  it => {\n",
        "    println(s\"Logger: ${it.getName}, Level enabled: ${it.getLevel}, DEBUG enabled? ${it.isDebugEnabled}\")\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Log Levels\n",
        "\n",
        "By default all **DEBUG** level is disabled for all loggers in Synapse Spark. Hence the DEBUG logs are not propogated to the Log Analytics workspace.\n",
        "\n",
        "NOTE: INFO is set as the default log level in Synapse spark. Because DEBUG has lower priority than INFO (`DEBUG < INFO < WARN < ERROR < FATAL`), all debug logs are not outputted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "//Pay attemtion to the Total Loggers count from previous cell output. It is 427\n",
        "//After this cell, the count should be 428\n",
        "val logger = org.apache.log4j.LogManager.getLogger(\"com.aravind.SparkAppOne\")\n",
        "logger.info(\"info message 1\")\n",
        "logger.debug(\"debug message 1\")\n",
        "logger.warn(\"warn message 1\")\n",
        "logger.error(\"error message 1\")\n",
        "logger.fatal(\"fatal message 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import scala.collection.JavaConversions._\n",
        "val currentLoggers=logRepo.getCurrentLoggers().asInstanceOf[java.util.Enumeration[Logger]].toSet\n",
        "\n",
        "println(s\"Total loggers: ${currentLoggers.size}\")\n",
        "currentLoggers.filter(l=> l.getName.contains(\"aravind\")).foreach(it => {\n",
        "    println(s\"Logger: ${it.getName}, Level enabled: ${it.getLevel}, DEBUG enabled? ${it.isDebugEnabled}\")\n",
        "  })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Enable DEBUG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "However, you would need DEBUG enabled on DEV and TEST environments and in PROD while debugging a specific issues. Spark uses Log4j for all logging. It provides a mechanism to set custome log levels programatically as shown below.\n",
        "\n",
        "```\n",
        "val myLogger = LogManager.getLogger(\"com.aravind.SparkAppOne\")\n",
        "myLogger.setLevel(Level.DEBUG)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "println(\"Log level BEFORE changing it\")\n",
        "currentLoggers.filter{_.getName.contains(\"com.aravind\")}.foreach{\n",
        "  it => {\n",
        "    println(s\"Logger: ${it.getName}, Level enabled: ${it.getLevel}, DEBUG enabled? ${it.isDebugEnabled}\")\n",
        "  }\n",
        "}\n",
        "\n",
        "println(\"Log level AFTER changing it\")\n",
        "val aravindLogger = LogManager.getLogger(\"com.aravind.SparkAppOne\")\n",
        "aravindLogger.setLevel(Level.DEBUG)\n",
        "\n",
        "currentLoggers.filter{_.getName.contains(\"com.aravind\")}.foreach{\n",
        "  it => {\n",
        "    println(s\"Logger: ${it.getName}, Level enabled: ${it.getLevel}, DEBUG enabled? ${it.isDebugEnabled}\")\n",
        "  }\n",
        "}\n",
        "\n",
        "logger.debug(\"DEBUG msg after enabling DEBUG level\")\n",
        "logger.info(\"INFO msg after enabling DEBUG level\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Appenders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val rootLogger = LogManager.getRootLogger\n",
        "\n",
        "import scala.collection.JavaConversions._\n",
        "val allAppenders = rootLogger.getAllAppenders().asInstanceOf[java.util.Enumeration[Appender]].toSet\n",
        "\n",
        "println(s\"Total Appenders: ${allAppenders.size}\")\n",
        "\n",
        "allAppenders.foreach{\n",
        "  it => {\n",
        "    println(s\"Appender: ${it.getName}, Layout: ${it.getLayout}, Filter: ${it.getFilter}\")\n",
        "    it.getLayout\n",
        "  }\n",
        "}\n",
        "\n",
        "val laAppender = rootLogger.getAppender(\"LogAnalyticsAppender\")\n",
        "println(\"LogAnalyticsAppender: \" +laAppender)\n",
        "\n",
        "val laLayout = laAppender.getLayout\n",
        "println(\"LogAnalyticsAppender.Layout: \"+laLayout)\n",
        "println(\"LogAnalyticsAppender.Layout contnetType: \" +laLayout.getContentType)\n",
        "println(\"LogAnalyticsAppender.Layout Header: \" +laLayout.getHeader)\n",
        "println(\"LogAnalyticsAppender.Layout Footer: \" +laLayout.getFooter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Custom Layout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "//val PATTERN = \"%d [%p|%c|%C{1}] %m%n\";\n",
        "val PATTERN = \"%d{ISO8601} %-5p [%t] %c{4} - %m%n\"\n",
        "aravindAppender.setLayout(new com.microsoft.azure.synapse.loganalytics.logging.JSONLayout(false))\n",
        "logger.info(\"INFO 1After setting new layout\")\n",
        "logger.debug(\"DEBUG 1After setting new layout\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import com.microsoft.azure.synapse.loganalytics.logging._\n",
        "\n",
        "val laAppender = rootLogger.getAppender(\"LogAnalyticsAppender\")\n",
        "val lal: JSONLayout = laAppender.getLayout().asInstanceOf[JSONLayout]\n",
        "println(lal.getLocationInfo)\n",
        "println(lal.getContentType)\n",
        "println(lal.getJsonConfiguration)\n",
        "println(lal.getHeader)\n",
        "println(lal.getFooter)\n",
        "//println(lal.format)\n",
        "//println((lal.getContentType)\n",
        "//println((lal.getHeader)\n",
        "// println((lal.getFooter)\n",
        "//println((lal.getJsonConfiguration)\n",
        "// println((lal.getLocationInfo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sparkContext.setLocalProperty(\"mdc.customMdcProp\", \"my value\")\n",
        "logger.info(\"Test custom MDC property\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Exception on Driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "throw new Exception(\"Exception from Driver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "try {\n",
        "    throw new Exception(\"Test Log error\")\n",
        "} catch {\n",
        "    //case ex: Exception => logger.error(ex)\n",
        "    case ex: Exception => logger.error(ex.getMessage, ex)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Exception on Worker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import org.apache.spark.sql.Row\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val numRdd = sc.parallelize(Seq(\n",
        "    Row(1,1), Row(1,0), Row(2,0)\n",
        "))\n",
        "\n",
        "val schema = StructType(Array(\n",
        "    StructField(\"a\",DoubleType,true),\n",
        "    StructField(\"b\",DoubleType,true)\n",
        "    ))\n",
        "\n",
        "val numDf = spark.createDataFrame(numRdd, schema)\n",
        "numDf.createOrReplaceTempView(\"number_tbl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "numDf.rdd.getNumPartitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "SELECT a, b from number_tbl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Reference\n",
        "\n",
        "- [Log4j 1.2 Manual](https://logging.apache.org/log4j/1.2/manual.html)\n",
        "- [Log4j 1.2 JavaDoc](https://logging.apache.org/log4j/1.2/apidocs/index.html)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}