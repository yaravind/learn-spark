{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "println(sc.version)\n",
        "val logger = org.apache.log4j.LogManager.getLogger(\"com.aravind.cust.satisfaction\")\n",
        "\n",
        "import org.apache.spark.sql._\n",
        "import org.apache.spark.sql.types._\n",
        "import scala.reflect.runtime.universe._\n",
        "\n",
        "import org.apache.spark.sql._\n",
        "import com.crealytics.spark.excel._\n",
        "import com.crealytics.spark.excel.WorkbookReader\n",
        "import com.microsoft.spark.notebook.msutils.MSFileInfo"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//tlfs@xxx.dfs.core.windows.net\n",
        "val defContainer = \"tlfs\"\n",
        "val defStorageAccount = \"xxx\"\n",
        "\n",
        "/*\n",
        "account_name = \"Your account name\"\n",
        "container_name = \"Your container name\"\n",
        "relative_path = \"Your path\"\n",
        "\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
        "*/\n",
        "\n",
        "//abfss://tlfs@xxx.dfs.core.windows.net/poc/planre/BSO/part-00000-4e7f73e0-67dd-4773-a872-b1855e2629f2-c000.snappy.parquet\n",
        "val storageAccount = \"poc\"\n",
        "val container = \"xxx\"\n",
        "val planReBsoPath = \"planre/BSO/part-00000-4e7f73e0-67dd-4773-a872-b1855e2629f2-c000.snappy.parquet\"\n",
        "val planReBsoAbsolutePath = s\"abfss://${storageAccount}@${container}.dfs.core.windows.net/${planReBsoPath}\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "  import java.time._\n",
        "\n",
        "  val files =   deepLs(new MSFileInfo(name = null, path = \"abfss://poc@xxx.dfs.core.windows.net/planre/BSO\", size = 0, isDir = true, isFile = false, System.currentTimeMillis()))\n",
        "  println(\"files: \"+files.length)\n",
        "  val fileDf = spark.createDataFrame(spark.sparkContext.parallelize(files))\n",
        "  //fileDf.show(truncate=false)\n",
        "  display(fileDf)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(s\"Reading from path: ${planReBsoAbsolutePath}\")\n",
        "val df = spark.read.parquet(planReBsoAbsolutePath)\n",
        "df.show(5)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2022 P09 Daily Service Level "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val srvcLevlPath = \"cust-srvc/2022 P09 Daily Service Level October 8-DB Worksheet-nonbinary.xlsx\"\n",
        "val srvcLevlAbsolutePath = s\"abfss://${storageAccount}@${container}.dfs.core.windows.net/${srvcLevlPath}\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "/*\n",
        "treatEmptyValuesAsNulls = false,  // Optional, default: true\n",
        "    setErrorCellsToFallbackValues = false, // Optional, default: false, where errors will be converted to null. If true, any ERROR cell values (e.g. #N/A) will be converted to the zero values of the column's data type.\n",
        "    usePlainNumberFormat = false,  // Optional, default: false. If true, format the cells without rounding and scientific notations\n",
        "    inferSchema = false,  // Optional, default: false\n",
        "    addColorColumns = true,  // Optional, default: false\n",
        "    timestampFormat = \"MM-dd-yyyy HH:mm:ss\",  // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]\n",
        "    maxRowsInMemory = 20,  // Optional, default None. If set, uses a streaming reader which can help with big files (will fail if used with xls format files)\n",
        "    maxByteArraySize = 2147483647,  // Optional, default None. See https://poi.apache.org/apidocs/5.0/org/apache/poi/util/IOUtils.html#setByteArrayMaxOverride-int-\n",
        "    tempFileThreshold = 10000000, // Optional, default None. Number of bytes at which a zip entry is regarded as too large for holding in memory and the data is put in a temp file instead\n",
        "    excerptSize = 10,  // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from\n",
        "    workbookPassword = \"pass\"  // Optional, default None. Requires unlimited strength JCE for older JVMs\n",
        "*/\n",
        "logger.info(s\"Reading from path: ${srvcLevlAbsolutePath}\")\n",
        "\n",
        "val srvcLevlDf = spark.read.excel(\n",
        "    header = true,  // Required\n",
        "    dataAddress = \"'DB'!A2\", // Optional, default: \"A1\"\n",
        "    inferSchema = true,\n",
        "    maxRowsInMemory=100000\n",
        "    \n",
        ")//.schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings\n",
        " .load(srvcLevlAbsolutePath)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "println(s\"Total rows: ${srvcLevlDf.count}\")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consolidated Customer Database"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val custDbPath = \"cust-srvc/MT Consolidated Customer Database-nonbinary.xlsx\"\n",
        "val custDbAbsolutePath = s\"abfss://${storageAccount}@${container}.dfs.core.windows.net/${custDbPath}\"\n",
        "\n",
        "logger.info(s\"Reading from path: ${custDbAbsolutePath}\")\n",
        "\n",
        "val custDbDf = spark.read\n",
        "    .excel(\n",
        "        header = true,  // Required\n",
        "        dataAddress = \"'OUTLET'!A1\", // Optional, default: \"A1\"\n",
        "        inferSchema = true,\n",
        "        maxRowsInMemory=200000)\n",
        "    .load(custDbAbsolutePath)\n",
        "    .cache()\n",
        "    \n",
        "println(s\"Total rows: ${custDbDf.count}\")"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val schema: StructType = custDbDf.schema\n",
        "\n",
        "addFileNameCol(custDbDf)(spark).select(\"OUTLET NO\", \"file_name\").show(2)\n",
        "//println(\"Infered schema for CustDbDf\")\n",
        "//custDbDf.printSchema()\n",
        "//getType(schema)\n",
        "\n",
        "//println(schema.prettyJson)\n",
        "\n",
        "//val renamedCustDbDf = cleanColumnNames(custDbDf)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val sheetNames = WorkbookReader( \n",
        "    Map(\"path\" -> custDbAbsolutePath, \"maxRowsInMemory\" -> \"100000\"), \n",
        "    spark.sparkContext.hadoopConfiguration)\n",
        "    .sheetNames"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}